{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "102b7879-f363-4bc6-aa27-6ed89f553e1a",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The paper *“On Warm-Starting Neural Network Training”* by Jordan T. Ash and Ryan P. Adams addresses a common challenge in machine learning scenarios where new data arrives periodically and requires updating the existing neural network model. The paper investigates the trade-offs between two retraining strategies:\n",
    "\n",
    "-   Cold-starting\n",
    "-   Warm-starting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7469c8e-f842-44b6-b979-003bee74dd5d",
   "metadata": {},
   "source": [
    "**Cold-starting** means training a new model from scratch using the old and new data, ignoring the existing model weights and biases. **Warm-starting** means training a new model using the old and new data, but initializing the model weights and biases from the existing model. Intuitively, warm-starting should be faster and more effective than cold-starting, since it leverages the previous knowledge of the model. However, the paper shows that warm-starting often leads to worse generalization performance than cold-starting, even though the final training losses are similar.\n",
    "\n",
    "The paper conducts several comprehensive experiments to compare the performance of warm-starting and cold-starting, demonstrating the trade-off between generalization and training time. They also vary the models, optimizers, and datasets used in the experiments to validate their findings.\n",
    "\n",
    "The paper contributes to the understanding of how to efficiently utilize deep learning resources in dynamic environments, where data is constantly changing and models need to be updated frequently. The paper also provides some insights into the optimization landscape of neural networks, and how different initialization strategies affect the learning dynamics.\n",
    "\n",
    "The paper proposes a simple but clever technique to overcome this problem, called the shrink and perturb trick. The idea is to shrink the existing model weights towards zero by multiplying them by a factor less than one, and then add some noise to them. This creates a new initialization point that is close to the existing model, but not too close to cause overfitting. The paper demonstrates that this technique can achieve better generalization performance than warm-starting, and faster convergence than cold-starting, in several machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdc75b4-d1b4-440b-bad0-0046f86571a5",
   "metadata": {},
   "source": [
    "If you are using colab click on this link to go to the next notebook: [Open in Colab](https://colab.research.google.com/github/mohammed183/ml-reproducibility-p1/blob/main/notebooks/02-Claims.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
