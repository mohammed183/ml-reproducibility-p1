{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2:\n",
    "\n",
    "In this experiment, we compare two methods of weight initialization: warm-starting and random initialization, for two models: **ResNet18** and **3-layer MLP** with tanh activation. We also compare two optimizers: **SGD** and **Adam**, for updating the weights based on the gradients. We use three image classification datasets: **CIFAR-10**, **CIFAR-100** and **SVHN**, and report the test accuracy of each model on each dataset. All models are trained using a mini-batch size of 128 and a learning rate of 0.001.\n",
    "\n",
    "We use the same components as in experiment one: the `get_loaders` function to get the required datasetâ€™s train and test loaders, the [**ResNet18**](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html) model from `torchvision.models`, and the [**SGD**](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) optimizer from `torch.optim`. We also introduce some new components:\n",
    "\n",
    "1.  The **CIFAR-100** dataset, which has 60,000 color images of 100 classes. To get the CIFAR-100 dataset, we pass the string `cifar100` as the dataset name argument to the `get_loaders` function.\n",
    "2.  The **SVHN** dataset, which has 73,257 color images of 10 classes of street view house numbers. To get the SVHN dataset, we pass the string `svhn` as the dataset name argument to the `get_loaders` function.\n",
    "3.  The `MLP` class that defines a 3-layer MLP model with a tanh activation function and a bias term.\n",
    "4.  The `torch.optim.Adam` class that implements the [**Adam**](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) optimizer, which is an adaptive learning rate method.\n",
    "5.  The `train_to_threshold` function that trains the model until it reaches 99% training accuracy, following the original paper."
   ],
   "id": "36d92f78-59ac-4686-992d-ec105b8d7a44"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We import the required packages as before."
   ],
   "id": "5472d0d0-c165-4cdd-bef8-3b221203dcaf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split, ConcatDataset\n",
    "from torchvision import transforms, datasets, models"
   ],
   "id": "fa76fec8-95ce-4b14-868b-7861fdb41ea2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "This is the same `get_loaders` function from Experiment 1"
   ],
   "id": "0574689c-65b0-4200-8506-9459fdf4a0ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(dataset=\"cifar10\", use_half_train=False, batch_size=128, dataset_portion=None):\n",
    "    \"\"\"\n",
    "    This loads the whole CIFAR-10 into memory and returns train and test data according to params\n",
    "    @param use_half_train (bool): return half the data or the whole train data\n",
    "    @param batch_size (int): batch size for training and testing\n",
    "    @param dataset_portion (double): portion of train data\n",
    "\n",
    "    @returns dict() with train and test data loaders with keys `train_loader`, `test_loader`\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalization using channel means\n",
    "    normalize_transform = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "    # Creating transform function\n",
    "    train_transform = transforms.Compose([transforms.ToTensor(), normalize_transform])\n",
    "        \n",
    "    # Test transformation function    \n",
    "    test_transform = transforms.Compose([transforms.ToTensor(), normalize_transform])\n",
    "    \n",
    "    # Check which dataset is required and load data from torchvision datasets\n",
    "    if dataset == 'cifar10':\n",
    "        original_train_dataset = datasets.CIFAR10(root=os.path.join('data', 'cifar10_data'),\n",
    "                                             train=True, transform=train_transform, download=True)\n",
    "        original_test_dataset = datasets.CIFAR10(root=os.path.join('data', 'cifar10_data'),\n",
    "                                             train=False, transform=test_transform, download=True)\n",
    "    elif dataset == 'cifar100':\n",
    "        original_train_dataset = datasets.CIFAR100(root=os.path.join('data', 'cifar100_data'),\n",
    "                                             train=True, transform=train_transform, download=True)\n",
    "        original_test_dataset = datasets.CIFAR100(root=os.path.join('data', 'cifar100_data'),\n",
    "                                             train=False, transform=test_transform, download=True)\n",
    "    else:\n",
    "        original_train_dataset = datasets.SVHN(root=os.path.join('data', 'svhn_data'),\n",
    "                                             split='train', transform=train_transform, download=True)\n",
    "        original_test_dataset = datasets.SVHN(root=os.path.join('data', 'svhn_data'),\n",
    "                                             split='test', transform=test_transform, download=True)\n",
    "    \n",
    "    # Check half data flag\n",
    "    if use_half_train:\n",
    "        print('Using Half Data')\n",
    "        dataset_portion = 0.5\n",
    "        \n",
    "    # Check if only a portion is required\n",
    "    if dataset_portion:\n",
    "        dataset_size = len(original_train_dataset)\n",
    "        split = int(np.floor((1 - dataset_portion) * dataset_size))\n",
    "        original_train_dataset, _ = random_split(original_train_dataset, [dataset_size - split, split])\n",
    "    \n",
    "    # Creating data loaders\n",
    "    loader_args = {\n",
    "        \"batch_size\": batch_size,\n",
    "    }\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=original_train_dataset,\n",
    "        shuffle=True,\n",
    "        **loader_args)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=original_test_dataset,\n",
    "        shuffle=False,\n",
    "        **loader_args)\n",
    "\n",
    "    return {\"train_loader\": train_loader,\n",
    "            \"test_loader\": test_loader}"
   ],
   "id": "b083e4f6-15c7-4614-ac10-58be36039028"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "The following is a class for a **multilayer perceptron** (MLP) model with several fully connected (fc) layers and a final fully connected layer for the logits output. The arguments are:\n",
    "\n",
    "-   `input_dim`: the input feature dimension.\n",
    "-   `num_classes`: the output class number.\n",
    "-   `hidden_units`: the hidden unit number for each fc layer we set the default as 100 dimension as mentioned in the appendix.\n",
    "-   `activation`: the activation function, either `tanh` or `relu`.\n",
    "-   `bias`: whether to use bias terms in the fc layers.\n",
    "\n",
    "The function returns an MLP model object that can be trained or tested. The forward method takes an input tensor x and returns an output tensor x with the logits values. The output tensor does not have a final activation function. This will be used to create the **3-layer MLP** model."
   ],
   "id": "a06f61d4-944d-4d9f-9d1a-983c1b33046e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class for a multilayer perceptron (MLP) model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=10, hidden_units=[100, 100, 100], activation='tanh', bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Check that the activation argument is valid\n",
    "        assert activation in ['tanh', 'relu'], \"Activation must be tanh or relu\"\n",
    "\n",
    "        # Assign the activation function based on the argument\n",
    "        if activation == 'tanh':\n",
    "            self.activation_function = torch.tanh\n",
    "        if activation == 'relu':\n",
    "            self.activation_function = torch.relu\n",
    "        \n",
    "        # Store num_classes and input_dim to be used in forward function\n",
    "        self.num_classes = num_classes\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # Initialize a variable to keep track of the last dimension of the layers\n",
    "        last_dim = input_dim\n",
    "        \n",
    "        # Initialize an empty list to store the fully connected (fc) layers\n",
    "        self.fcs = []\n",
    "        \n",
    "        # Loop through the hidden units argument and create fc layers with the given dimensions and bias\n",
    "        for i, n_h in enumerate(hidden_units):\n",
    "            self.fcs.append(nn.Linear(last_dim, n_h, bias=bias))\n",
    "            # Register the fc layer as a submodule with a name\n",
    "            self.add_module(f\"hidden_layer_{i}\", self.fcs[-1])\n",
    "            # Update the last dimension to match the output dimension of the fc layer\n",
    "            last_dim = n_h\n",
    "            \n",
    "        # Create a final fc layer for the logits output with the number of classes and bias\n",
    "        self.logit_fc = nn.Linear(last_dim, self.num_classes, bias=bias)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Reshape the input x to have a batch size and an input dimension\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        \n",
    "        # Loop through the fc layers and apply them to x with the activation function\n",
    "        for fc in self.fcs:\n",
    "            x = fc(x)\n",
    "            x = self.activation_function(x)\n",
    "            \n",
    "        # Apply the final fc layer to x and return it as the output\n",
    "        x = self.logit_fc(x)\n",
    "        \n",
    "        # x is returned without adding the final activation\n",
    "        return x"
   ],
   "id": "35539682-7622-49fe-b2c4-462b33900458"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "The following cell defines two functions that perform one epoch of training or evaluation on a data loader. They return the average loss and accuracy of the model. These functions will be used in the training function to run epoch by epoch."
   ],
   "id": "28027266-7918-4bdf-aa2f-a357f0a4153a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_dataloader(device, criterion, model, dataloader):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a given data loader and return the average loss and accuracy.\n",
    "\n",
    "    Parameters:\n",
    "    device: the device (cpu or gpu) to use for computation\n",
    "    criterion: the loss function to use\n",
    "    model: the model to evaluate\n",
    "    dataloader: the data loader to iterate over the data\n",
    "\n",
    "    Returns:\n",
    "    loss: the average loss over the data loader\n",
    "    accuracy: the average accuracy over the data loader\n",
    "    \"\"\"\n",
    "    # Lists to store accuracy and loss\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    \n",
    "    for batch_idx, (data_x, data_y) in enumerate(dataloader): \n",
    "        data_x = data_x.to(device) \n",
    "        data_y = data_y.to(device)\n",
    "        \n",
    "        # get the model output for the input data\n",
    "        model_y = model(data_x) \n",
    "        \n",
    "        # compute the loss and accuracy\n",
    "        loss = criterion(model_y, data_y)\n",
    "        batch_accuracy = get_accuracy(model_y, data_y)\n",
    "        \n",
    "        # append accuracy and loss to lists\n",
    "        accuracies.append(batch_accuracy.item()) \n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # compute average loss and accuracy\n",
    "    loss = np.mean(losses) \n",
    "    accuracy = np.mean(accuracies) \n",
    "    return loss, accuracy \n",
    "\n",
    "\n",
    "def train_one_epoch(device, model, optimizer, criterion, dataloader):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch on a given training data loader and return the average loss and accuracy.\n",
    "\n",
    "    Parameters:\n",
    "    device: the device (cpu or gpu) to use for computation\n",
    "    model: the model to train\n",
    "    optimizer: the optimizer to use for updating the weights\n",
    "    criterion: the loss function to use\n",
    "    train_dataloader: the training data loader to iterate over the training data\n",
    "\n",
    "    Returns:\n",
    "    train_loss: the average loss over the training data loader\n",
    "    train_accuracy: the average accuracy over the training data loader\n",
    "    \"\"\"\n",
    "    # Lists to store accuracy and loss\n",
    "    accuracies = []\n",
    "    losses = [] \n",
    "    \n",
    "    for batch_idx, (data_x, data_y) in enumerate(dataloader):\n",
    "        data_x = data_x.to(device) \n",
    "        data_y = data_y.to(device) \n",
    "        \n",
    "         # reset the gradients of the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # get the model output for the input data\n",
    "        model_y = model(data_x)\n",
    "        \n",
    "        # compute the loss and accuracy\n",
    "        loss = criterion(model_y, data_y)\n",
    "        batch_accuracy = get_accuracy(model_y, data_y)\n",
    "        \n",
    "        # compute the gradients and update model parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # append accuracy and loss to lists\n",
    "        accuracies.append(batch_accuracy.item()) \n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # compute average loss and accuracy\n",
    "    loss = np.mean(losses) \n",
    "    accuracy = np.mean(accuracies) \n",
    "    return loss, accuracy \n"
   ],
   "id": "ec122d7e-ffa3-4bb6-bdac-31cfe3ce19c4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "The `train_to_threshold` function is the same as `train_model_exp1` functions except that it trains the model until a certain `training_threshold` or until the change in the training accuracy doesnâ€™t exceed `convergence_change_threshold` for certain number of epochs. The new parameter introduced are:\n",
    "\n",
    "-   `train_threshold`: The training accuracy at which the model stops training.\n",
    "-   `convergence_change_threshold`: The minimum accuracy change to continue training.\n",
    "-   `convergence_epochs`: The maximum number of epochs allowed with insufficient change in training accuracy before stopping the training."
   ],
   "id": "a18df5c2-035b-4dfe-8e82-93dfb799cd0b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_to_threshold(title='warm', lr=0.001, checkpoint=None, use_half_data=False, convergence_epochs=4,\n",
    "                       train_threshold=0.5, convergence_change_threshold=0.002, random_seed=42):\n",
    "    # use gpu if available ( change device id if needed )\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "        print(\"CUDA Recognized\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Get the dataset\n",
    "    loaders = get_loaders(dataset=\"cifar10\", use_half_train=use_half_data)\n",
    "\n",
    "    # Get the model\n",
    "    model = models.resnet18(num_classes=10).to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    # Create the loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Get model from checkpoint\n",
    "    if checkpoint is not None:\n",
    "        model.load_state_dict(torch.load(checkpoint, map_location=device)['model'])\n",
    "\n",
    "    print(f\"Training {'warm-starting' if checkpoint is not None else 'random initialized'} \" \\\n",
    "        f\"ResNet-18 model with SGD optimizer on {'50%' if use_half_data else '100%'} of cifar10 dataset\")\n",
    "\n",
    "    # initialize training varaibles\n",
    "    train_accuracies = []\n",
    "    stop_indicator = False\n",
    "    model_name = 'resnet18'\n",
    "    epoch = 0\n",
    "    # Train until convergence or stop condition is met\n",
    "    while(not stop_indicator):\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"\\t Training in epoch {epoch + 1} \\t\")\n",
    "        # Train for one epoch and get loss and accuracy\n",
    "        train_loss, train_accuracy = train_one_epoch(device, model, optimizer, criterion, loaders['train_loader'])\n",
    "\n",
    "        # Append training accuracy to list\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        epoch += 1\n",
    "        # Check if training accuracy is above a threshold\n",
    "        if train_accuracy >= train_threshold:\n",
    "            print(f\"Convergence codition met. Training accuracy > {train_threshold}\")\n",
    "            stop_indicator = True\n",
    "\n",
    "        # Check if training accuracy has stopped improving for a number of epochs\n",
    "        if len(train_accuracies) >= convergence_epochs:\n",
    "            if np.std(train_accuracies[-convergence_epochs:]) < convergence_change_threshold:\n",
    "                print(f\"\\tConvergence codition met. Training accuracy = {train_accuracy} stopped improving\")\n",
    "                stop_indicator = True\n",
    "\n",
    "    # Evaluate on test set and get loss and accuracy\n",
    "    test_loss, test_accuracy =  eval_on_dataloader(device, criterion, model, loaders['test_loader'])\n",
    "    print(f\"\\tTest accuracy = {test_accuracy}\")\n",
    "\n",
    "    # Save the model if will be used for warm-starting\n",
    "    if use_half_data:\n",
    "        # Create directory in exp with experiment title\n",
    "        experiment_dir = os.path.join('experiments/exp2', title)\n",
    "        os.makedirs(experiment_dir, exist_ok=True)\n",
    "        \n",
    "        # save the model\n",
    "        model_name = model_name+'-sgd'\n",
    "        model_directory =  os.path.join(experiment_dir, f'{model_name}.pt')\n",
    "        torch.save({\n",
    "            'model': model.state_dict()\n",
    "        }, model_directory)\n",
    "\n",
    "        print(f\"Model saved to checkpoint: {model_directory}\")\n",
    "\n",
    "    return test_accuracy * 100 "
   ],
   "id": "3a08e355-0857-43a9-8963-28b45bde874d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "The ResNet-18 model is trained on the CIFAR-10 dataset using an SGD optimizer and the `train_to_threshold` function. It is trained with warm-start and random initialization."
   ],
   "id": "73a2de2a-fccf-4be6-9ba1-e0598e23236b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to save all results\n",
    "overal_result = {}\n",
    "\n",
    "# train on full data with random initialization\n",
    "random_init = train_to_threshold(title='resnet-sgd-cifar10', train_threshold=0.99)"
   ],
   "id": "65aadcce-1e5e-412c-8765-14b3cc68c831"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on half data\n",
    "_ = train_to_threshold(title='resnet-sgd-cifar10', train_threshold=0.99, use_half_data=True)\n",
    "\n",
    "# train on full data with warm-starting\n",
    "warm_start = train_to_threshold(title='resnet-sgd-cifar10', train_threshold=0.99,\n",
    "                                     checkpoint='experiments/exp2/resnet-sgd-cifar10/resnet18-sgd.pt')"
   ],
   "id": "7baf442d-8cee-42f4-af43-61fb9e7b2f7b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the difference between random and warm-start models using sgd optimizer on CIFAR-10\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overal_result['resnet-sgd-cifar10'] = [random_init, warm_start, diff]"
   ],
   "id": "c0735245-88bb-495f-b27f-7835878e291c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We extend this experiment by training the same model with the Adam optimizer instead of SGD. We add a new parameter `optimizer_name` to select the optimizer for the model."
   ],
   "id": "023df3ee-26ac-4671-bc51-d7681be388e4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_to_threshold(title='warm', lr=0.001, checkpoint=None, \n",
    "                       use_half_data=False, optimizer_name='adam', convergence_epochs=4,\n",
    "                       train_threshold=0.5, convergence_change_threshold=0.002, random_seed=42):\n",
    "    # use gpu if available ( change device id if needed )\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "        print(\"CUDA Recognized\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Get the dataset\n",
    "    loaders = get_loaders(dataset=\"cifar10\", use_half_train=use_half_data)\n",
    "\n",
    "    # Get the model\n",
    "    model = models.resnet18(num_classes=10).to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    if optimizer_name == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Create the loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Get model from checkpoint\n",
    "    if checkpoint is not None:\n",
    "        model.load_state_dict(torch.load(checkpoint, map_location=device)['model'])\n",
    "\n",
    "    print(f\"Training {'warm-starting' if checkpoint is not None else 'random initialized'} ResNet-18 model \" \\\n",
    "            f\"with {optimizer_name.upper()} optimizer on {'50%' if use_half_data else '100%'} of cifar10 dataset\")\n",
    "\n",
    "    # initialize training varaibles\n",
    "    train_accuracies = []\n",
    "    stop_indicator = False\n",
    "    model_name = 'resnet18'\n",
    "    epoch = 0\n",
    "    # Train until convergence or stop condition is met\n",
    "    while(not stop_indicator):\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"\\t Training in epoch {epoch + 1} \\t\")\n",
    "        # Train for one epoch and get loss and accuracy\n",
    "        train_loss, train_accuracy = train_one_epoch(device, model, optimizer, criterion, loaders['train_loader'])\n",
    "\n",
    "        # Append training accuracy to list\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        epoch += 1\n",
    "        # Check if training accuracy is above a threshold\n",
    "        if train_accuracy >= train_threshold:\n",
    "            print(f\"Convergence codition met. Training accuracy > {train_threshold}\")\n",
    "            stop_indicator = True\n",
    "\n",
    "        # Check if training accuracy has stopped improving for a number of epochs\n",
    "        if len(train_accuracies) >= convergence_epochs:\n",
    "            if np.std(train_accuracies[-convergence_epochs:]) < convergence_change_threshold:\n",
    "                print(f\"\\tConvergence codition met. Training accuracy = {train_accuracy} stopped improving\")\n",
    "                stop_indicator = True\n",
    "\n",
    "    # Evaluate on test set and get loss and accuracy\n",
    "    test_loss, test_accuracy =  eval_on_dataloader(device, criterion, model, loaders['test_loader'])\n",
    "    print(f\"\\tTest accuracy = {test_accuracy}\")\n",
    "\n",
    "    # Save the model if will be used for warm-starting\n",
    "    if use_half_data:\n",
    "        # Create directory in exp with experiment title\n",
    "        experiment_dir = os.path.join('experiments/exp2', title)\n",
    "        os.makedirs(experiment_dir, exist_ok=True)\n",
    "        \n",
    "        # save the model\n",
    "        model_name = model_name+'-'+optimizer_name\n",
    "        model_directory =  os.path.join(experiment_dir, f'{model_name}.pt')\n",
    "        torch.save({\n",
    "            'model': model.state_dict()\n",
    "        }, model_directory)\n",
    "\n",
    "        print(f\"Model saved to checkpoint: {model_directory}\")\n",
    "\n",
    "    return test_accuracy * 100"
   ],
   "id": "58816e2d-4a48-4ad0-87da-d6ef48b0e7a6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We repeat the training of the same models with the Adam optimizer instead of SGD."
   ],
   "id": "fc8ac581-a025-4233-8fda-2a609599e6f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on full data with random initialization but with Adam\n",
    "random_init = train_to_threshold(title='resnet-adam-cifar10', train_threshold=0.99, optimizer_name='adam')\n",
    "\n",
    "# train on half data\n",
    "_ = train_to_threshold(title='resnet-adam-cifar10', train_threshold=0.99,\n",
    "                                             optimizer_name='adam', use_half_data=True)\n",
    "\n",
    "# train on full data with warm-starting but with Adam\n",
    "warm_start = train_to_threshold(title='resnet-adam-cifar10', train_threshold=0.99, optimizer_name='adam',\n",
    "                                        checkpoint='experiments/exp2/resnet-adam-cifar10/resnet18-adam.pt')\n",
    "\n",
    "# get the difference between random and warm-start models using Adam optimizer on CIFAR-10\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overal_result['resnet-adam-cifar10'] = [random_init, warm_start, diff]"
   ],
   "id": "b0bbb460-5ee8-4308-89db-d0e85a8d3834"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We also experiment with the MLP model instead of the ResNet. We introduce a parameter `model_name` to select the model for the training."
   ],
   "id": "71f47cd8-110d-4a82-91b8-169feee08bc1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_to_threshold(title='warm', lr=0.001, checkpoint=None, use_half_data=False,\n",
    "                        optimizer_name='adam', model_name='resnet18', convergence_epochs=4,\n",
    "                        train_threshold=0.5, convergence_change_threshold=0.002, random_seed=42):\n",
    "    # use gpu if available ( change device id if needed )\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "        print(\"CUDA Recognized\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Get the dataset\n",
    "    loaders = get_loaders(dataset=\"cifar10\", use_half_train=use_half_data)\n",
    "\n",
    "    # Get the model\n",
    "    if model_name == 'resnet18':\n",
    "        model = models.resnet18(num_classes=10).to(device)\n",
    "    else:\n",
    "        model = MLP( input_dim = 32 * 32 * 3, num_classes=10).to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    if optimizer_name == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Create the loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Get model from checkpoint\n",
    "    if checkpoint is not None:\n",
    "        model.load_state_dict(torch.load(checkpoint, map_location=device)['model'])\n",
    "\n",
    "    print(f\"Training {'warm-starting' if checkpoint is not None else 'random initialized'} {model_name} model \" \\\n",
    "            f\"with {optimizer_name.upper()} optimizer on {'50%' if use_half_data else '100%'} of cifar10 dataset\")\n",
    "\n",
    "    # initialize training varaibles\n",
    "    train_accuracies = []\n",
    "    stop_indicator = False\n",
    "    epoch = 0\n",
    "    # Train until convergence or stop condition is met\n",
    "    while(not stop_indicator):\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"\\t Training in epoch {epoch + 1} \\t\")\n",
    "        # Train for one epoch and get loss and accuracy\n",
    "        train_loss, train_accuracy = train_one_epoch(device, model, optimizer, criterion, loaders['train_loader'])\n",
    "\n",
    "        # Append training accuracy to list\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        epoch += 1\n",
    "        # Check if training accuracy is above a threshold\n",
    "        if train_accuracy >= train_threshold:\n",
    "            print(f\"Convergence codition met. Training accuracy > {train_threshold}\")\n",
    "            stop_indicator = True\n",
    "\n",
    "        # Check if training accuracy has stopped improving for a number of epochs\n",
    "        if len(train_accuracies) >= convergence_epochs:\n",
    "            if np.std(train_accuracies[-convergence_epochs:]) < convergence_change_threshold:\n",
    "                print(f\"\\tConvergence codition met. Training accuracy = {train_accuracy} stopped improving\")\n",
    "                stop_indicator = True\n",
    "\n",
    "    # Evaluate on test set and get loss and accuracy\n",
    "    test_loss, test_accuracy =  eval_on_dataloader(device, criterion, model, loaders['test_loader'])\n",
    "    print(f\"\\tTest accuracy = {test_accuracy}\")\n",
    "\n",
    "    # Save model if it is needed for warm-starting\n",
    "    if use_half_data:\n",
    "        # Create directory in exp with experiment title\n",
    "        experiment_dir = os.path.join('experiments/exp2', title)\n",
    "        os.makedirs(experiment_dir, exist_ok=True)\n",
    "        \n",
    "        # save the model\n",
    "        model_name = model_name+'-'+optimizer_name\n",
    "        model_directory =  os.path.join(experiment_dir, f'{model_name}.pt')\n",
    "        torch.save({\n",
    "            'model': model.state_dict()\n",
    "        }, model_directory)\n",
    "\n",
    "        print(f\"Model saved to checkpoint: {model_directory}\")\n",
    "\n",
    "    return test_accuracy * 100"
   ],
   "id": "c55bb944-6ddc-4beb-ae7d-0a2f268c6753"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We use warm-starting and the SGD optimizer to train the MLP model in the next cell."
   ],
   "id": "5a5c057c-8ae0-405e-9806-3678833edf20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train MLP model on full data with random initialization and SGD optimizer\n",
    "random_init = train_to_threshold(title='mlp-sgd-cifar10', train_threshold=0.99, \n",
    "                                 optimizer_name='sgd', model_name='mlp')\n",
    "\n",
    "# train MLP model on half data\n",
    "_ = train_to_threshold(title='mlp-sgd-cifar10', train_threshold=0.99, \n",
    "                       optimizer_name='sgd', model_name='mlp', use_half_data=True)\n",
    "\n",
    "# train MLP on full data with warm-starting and SGD optimizer\n",
    "warm_start = train_to_threshold(title='mlp-sgd-cifar10', train_threshold=0.99, optimizer_name='sgd', \n",
    "                                model_name='mlp', checkpoint='experiments/exp2/mlp-sgd-cifar10/mlp-sgd.pt')\n",
    "\n",
    "# get the difference between random and warm-start MLP models using SGD optimizer on CIFAR-10\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overal_result['mlp-sgd-cifar10'] = [random_init, warm_start, diff]"
   ],
   "id": "baa9cde6-6c8d-43cf-a61f-74272c3f522e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "In the next cell, we train the MLP model with warm-starting and the Adam optimizer."
   ],
   "id": "843cb2ac-2269-48e3-930b-014d7627e940"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train MLP model on full data with random initialization\n",
    "random_init= train_to_threshold(title='mlp-adam-cifar10', train_threshold=0.99, \n",
    "                                optimizer_name='adam', model_name='mlp')\n",
    "\n",
    "# train MLP mode on half data\n",
    "_ = train_to_threshold(title='mlp-adam-cifar10', train_threshold=0.99, \n",
    "                       optimizer_name='adam', model_name='mlp', use_half_data=True)\n",
    "\n",
    "# train MLP on full data with warm-starting\n",
    "warm_start = train_to_threshold(title='mlp-adam-cifar10', train_threshold=0.99, optimizer_name='adam',\n",
    "                                model_name='mlp', checkpoint='experiments/exp2/mlp-adam-cifar10/mlp-adam.pt')\n",
    "\n",
    "# get the difference between random and warm-start MLP models using Adam optimizer on CIFAR-10\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overal_result['mlp-adam-cifar10'] = [random_init, warm_start, diff]"
   ],
   "id": "685d39ce-f0c2-47ab-b223-b416b712d6ac"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "Finaly we extend this to allow using the SVHN and CIFAR-100 datasets by adding a parameter `dataset` to specify the dataset we would like to use."
   ],
   "id": "14d6c823-0892-4e6f-bea3-4f9356e8ba09"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_to_threshold(title='warm', dataset='cifar10', lr=0.001, checkpoint=None, use_half_data=False,\n",
    "                       optimizer_name='adam', model_name='resnet18', convergence_epochs=4,\n",
    "                       train_threshold=0.5, convergence_change_threshold=0.002, random_seed=42):\n",
    "    # use gpu if available ( change device id if needed )\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "        print(\"CUDA Recognized\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Get the dataset\n",
    "    loaders = get_loaders(dataset=dataset, use_half_train=use_half_data)\n",
    "\n",
    "    # Define the number of classes\n",
    "    num_classes = 10\n",
    "    if dataset == 'cifar100':\n",
    "        num_classes=100\n",
    "\n",
    "    # Get the model\n",
    "    if model_name == 'resnet18':\n",
    "        model = models.resnet18(num_classes=num_classes).to(device)\n",
    "    else:\n",
    "        model = MLP( input_dim = 32 * 32 * 3, num_classes=num_classes).to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    if optimizer_name == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Create the loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Get model from checkpoint\n",
    "    if checkpoint is not None:\n",
    "        model.load_state_dict(torch.load(checkpoint, map_location=device)['model'])\n",
    "\n",
    "    print(f\"Training {'random initialized' if checkpoint is None else 'warm-starting'} {model_name} model wtih \" \\\n",
    "            f\"{optimizer_name.upper()} optimizer on {'50%' if use_half_data else '100%'} of {dataset} dataset\")\n",
    "\n",
    "    # initialize training variables\n",
    "    train_accuracies = []\n",
    "    stop_indicator = False\n",
    "    epoch = 0\n",
    "    # Train until convergence or stop condition is met\n",
    "    while(not stop_indicator):\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"\\t Training in epoch {epoch + 1} \\t\")\n",
    "        # Train for one epoch and get loss and accuracy\n",
    "        train_loss, train_accuracy = train_one_epoch(device, model, optimizer, criterion, loaders['train_loader'])\n",
    "\n",
    "        # Append training accuracy to list\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        epoch += 1\n",
    "        # Check if training accuracy is above a threshold\n",
    "        if train_accuracy >= train_threshold:\n",
    "            print(f\"Convergence codition met. Training accuracy > {train_threshold}\")\n",
    "            stop_indicator = True\n",
    "\n",
    "        # Check if training accuracy has stopped improving for a number of epochs\n",
    "        if len(train_accuracies) >= convergence_epochs:\n",
    "            if np.std(train_accuracies[-convergence_epochs:]) < convergence_change_threshold:\n",
    "                print(f\"\\tConvergence codition met. Training accuracy = {train_accuracy} stopped improving\")\n",
    "                stop_indicator = True\n",
    "\n",
    "    # Evaluate on test set and get loss and accuracy\n",
    "    test_loss, test_accuracy =  eval_on_dataloader(device, criterion, model, loaders['test_loader'])\n",
    "    print(f\"\\tTest accuracy = {test_accuracy}\")\n",
    "\n",
    "    # Save the model if it will be used for warm-starting\n",
    "    if use_half_data:\n",
    "        # Create directory in exp with experiment title\n",
    "        experiment_dir = os.path.join('experiments/exp2', title)\n",
    "        os.makedirs(experiment_dir, exist_ok=True)\n",
    "\n",
    "        # save the model\n",
    "        model_name = model_name+'-'+optimizer_name\n",
    "        model_directory =  os.path.join(experiment_dir, f'{model_name}.pt')\n",
    "        torch.save({\n",
    "            'model': model.state_dict()\n",
    "        }, model_directory)\n",
    "\n",
    "        print(f\"Model saved to checkpoint: {model_directory}\")\n",
    "\n",
    "    return test_accuracy * 100"
   ],
   "id": "21defa07-db84-4af7-a34d-d3ecd95dabfb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We repeat all the previous for the CIFAR-100 dataset using the ResNet model with different optimizers."
   ],
   "id": "914c48da-d66f-489b-94ec-4e12b0b5b753"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Resnet model on full CIFAR-100 data with random initialization and SGD optimizer\n",
    "random_init = train_to_threshold(title='resnet-sgd-cifar100', dataset='cifar100', train_threshold=0.99,\n",
    "                                       optimizer_name='sgd', model_name='resnet18')\n",
    "\n",
    "# train on half CIFAR-100 data\n",
    "_ = train_to_threshold(title='resnet-sgd-cifar100', dataset='cifar100', train_threshold=0.99, optimizer_name='sgd',\n",
    "                   model_name='resnet18', use_half_data=True)\n",
    "\n",
    "# train Resnet model on full CIFAR-100 data with warm-starting and SGD optimizer\n",
    "warm_start = train_to_threshold(title='resnet-sgd-cifar100', dataset='cifar100', train_threshold=0.99, \n",
    "                                optimizer_name='sgd', model_name='resnet18', \n",
    "                                checkpoint='experiments/exp2/resnet-sgd-cifar100/resnet18-sgd.pt')\n",
    "\n",
    "# get the difference between random and warm-start ResNet models using SGD optimizer on CIFAR-100\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overal_result['resnet-sgd-cifar100'] = [random_init, warm_start, diff]\n",
    "\n",
    "# train Resnet model on full CIFAR-100 data with random initialization and Adam optimizer\n",
    "random_init = train_to_threshold(title='resnet-adam-cifar100', dataset='cifar100', train_threshold=0.99,\n",
    "                                            optimizer_name='adam', model_name='resnet18')\n",
    "\n",
    "# train on half CIFAR-100 data\n",
    "_ = train_to_threshold(title='resnet-adam-cifar100', dataset='cifar100', train_threshold=0.99, \n",
    "                       optimizer_name='adam', model_name='resnet18', use_half_data=True)\n",
    "\n",
    "# train Resnet model on full CIFAR-100 data with warm-starting and Adam optimizer\n",
    "warm_start = train_to_threshold(title='resnet-adam-cifar100', dataset='cifar100', train_threshold=0.99, \n",
    "                                optimizer_name='adam', model_name='resnet18', \n",
    "                                checkpoint='experiments/exp2/resnet-adam-cifar100/resnet18-adam.pt')\n",
    "\n",
    "# get the difference between random and warm-start ResNet models using Adam optimizer on CIFAR-100\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overal_result['resnet-adam-cifar100'] = [random_init, warm_start, diff]"
   ],
   "id": "b5ef2f5a-1319-42bb-983f-ef2c03d9e4d6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We repeat all the previous for the CIFAR-100 dataset using the MLP model with different optimizers."
   ],
   "id": "8f73db66-d8ae-469a-b7ef-038515efe12d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train MLP model on full CIFAR-100 data with random initialization and SGD optimizer\n",
    "random_init = train_to_threshold(title='mlp-sgd-cifar100', dataset='cifar100', train_threshold=0.99,\n",
    "                                             optimizer_name='sgd', model_name='mlp')\n",
    "\n",
    "# train on half CIFAR-100 data\n",
    "_ = train_to_threshold(title='mlp-sgd-cifar100', dataset='cifar100', train_threshold=0.99,\n",
    "                       optimizer_name='sgd', model_name='mlp', use_half_data=True)\n",
    "\n",
    "# train MLP model on full CIFAR-100 data with warm-starting and SGD optimizer\n",
    "warm_start = train_to_threshold(title='mlp-sgd-cifar100', dataset='cifar100', train_threshold=0.99, \n",
    "                                optimizer_name='sgd', model_name='mlp', \n",
    "                                checkpoint='experiments/exp2/mlp-sgd-cifar100/mlp-sgd.pt')\n",
    "\n",
    "# get the difference between random and warm-start MLP models using SGD optimizer on CIFAR-100\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overal_result['mlp-sgd-cifar100'] = [random_init, warm_start, diff]\n",
    "\n",
    "# train MLP model on full CIFAR-100 data with random initialization and Adam optimizer\n",
    "random_init = train_to_threshold(title='mlp-adam-cifar100', dataset='cifar100',train_threshold=0.99,\n",
    "                                              optimizer_name='adam', model_name='mlp')\n",
    "\n",
    "# train on half CIFAR-100 data\n",
    "_ = train_to_threshold(title='mlp-adam-cifar100', dataset='cifar100', train_threshold=0.99, \n",
    "                       optimizer_name='adam', model_name='mlp', use_half_data=True)\n",
    "\n",
    "# train MLP model on full CIFAR-100 data with warm-starting and Adam optimizer\n",
    "warm_start = train_to_threshold(title='mlp-adam-cifar100', dataset='cifar100', train_threshold=0.99, \n",
    "                                optimizer_name='adam', model_name='mlp', \n",
    "                                checkpoint='experiments/exp2/mlp-adam-cifar100/mlp-adam.pt')\n",
    "\n",
    "# get the difference between random and warm-start MLP models using Adam optimizer on CIFAR-100\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overal_result['mlp-adam-cifar100'] = [random_init, warm_start, diff]"
   ],
   "id": "3bd7750c-3365-4512-ac19-eee95cb2d9cb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We create the previous for the SVHN dataset using the ResNet model with different optimizers."
   ],
   "id": "e399c44a-bd7b-4c66-b4ee-898a981cbacc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train ResNet model on full SVHN data with random initialization and SGD optimizer\n",
    "random_init = train_to_threshold(title='resnet-sgd-svhn', dataset='svhn', train_threshold=0.99,\n",
    "                                       optimizer_name='sgd', model_name='resnet18')\n",
    "\n",
    "# train on half SVHN data\n",
    "_ = train_to_threshold(title='resnet-sgd-svhn', dataset='svhn', train_threshold=0.99, \n",
    "                       optimizer_name='sgd', model_name='resnet18', use_half_data=True)\n",
    "\n",
    "# train ResNet model on full SVHN data with warm-starting and SGD optimizer\n",
    "warm_start = train_to_threshold(title='resnet-sgd-svhn', dataset='svhn', train_threshold=0.99, \n",
    "                                optimizer_name='sgd', model_name='resnet18',\n",
    "                                checkpoint='experiments/exp2/resnet-sgd-svhn/resnet18-sgd.pt')\n",
    "\n",
    "# store the difference between random and warm-start ResNet models using SGD optimizer on SVHN\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overal_result['resnet-sgd-svhn'] = [random_init, warm_start, diff]\n",
    "\n",
    "# train ResNet model on full SVHN data with random initialization and Adam optimizer\n",
    "random_init = train_to_threshold(title='resnet-adam-svhn', dataset='svhn', train_threshold=0.99,\n",
    "                                        optimizer_name='adam', model_name='resnet18')\n",
    "\n",
    "# train on half SVHN data\n",
    "_ = train_to_threshold(title='resnet-adam-svhn', dataset='svhn', train_threshold=0.99, \n",
    "                       optimizer_name='adam', model_name='resnet18', use_half_data=True)\n",
    "\n",
    "# train ResNet model on full SVHN data with warm-starting and Adam optimizer\n",
    "warm_start = train_to_threshold(title='resnet-adam-svhn', dataset='svhn', train_threshold=0.99, \n",
    "                                optimizer_name='adam', model_name='resnet18', \n",
    "                                checkpoint='experiments/exp2/resnet-adam-svhn/resnet18-adam.pt')\n",
    "\n",
    "# store the difference between random and warm-start ResNet models using Adam optimizer on SVHN\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overal_result['resnet-adam-svhn'] = [random_init, warm_start, diff]"
   ],
   "id": "79e2371d-c02c-4c20-8ed1-fb3f14277b76"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We create the previous for the SVHN dataset using the MLP model with different optimizers."
   ],
   "id": "222801ec-6406-45a8-8405-b881a00f5321"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train MLP model on full SVHN data with random initialization and SGD optimizer\n",
    "random_init = train_to_threshold(title='mlp-sgd-svhn', dataset='svhn', train_threshold=0.99,\n",
    "                                             optimizer_name='sgd', model_name='mlp')\n",
    "\n",
    "# train on half SVHN data\n",
    "_ = train_to_threshold(title='mlp-sgd-svhn', dataset='svhn', train_threshold=0.99, \n",
    "                       optimizer_name='sgd', model_name='mlp', use_half_data=True)\n",
    "\n",
    "# train MLP model on full SVHN data with warm-starting and SGD optimizer\n",
    "warm_start = train_to_threshold(title='mlp-sgd-svhn', dataset='svhn', train_threshold=0.99, \n",
    "                                optimizer_name='sgd', model_name='mlp',\n",
    "                                checkpoint='experiments/exp2/mlp-sgd-svhn/mlp-sgd.pt')\n",
    "\n",
    "# get the difference between random and warm-start MLP models using SGD optimizer on SVHN\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overal_result['mlp-sgd-svhn'] = [random_init, warm_start, diff]\n",
    "\n",
    "# train MLP model on full SVHN data with random initialization and Adam optimizer\n",
    "random_init = train_to_threshold(title='mlp-adam-svhn', dataset='svhn', train_threshold=0.99,\n",
    "                                              optimizer_name='adam', model_name='mlp')\n",
    "\n",
    "# train on half SVHN data\n",
    "_ = train_to_threshold(title='mlp-adam-svhn', dataset='svhn', train_threshold=0.99, \n",
    "                       optimizer_name='adam', model_name='mlp', use_half_data=True)\n",
    "\n",
    "# train MLP model on full SVHN data with warm-starting and Adam optimizer\n",
    "warm_start = train_to_threshold(title='mlp-adam-svhn', dataset='svhn', train_threshold=0.99, \n",
    "                                optimizer_name='adam', model_name='mlp',\n",
    "                                checkpoint='experiments/exp2/mlp-adam-svhn/mlp-adam.pt')\n",
    "\n",
    "# get the difference between random and warm-start MLP models using Adam optimizer on SVHN\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overal_result['mlp-adam-svhn'] = [random_init, warm_start, diff]"
   ],
   "id": "ff276607-b3fe-49b8-b76e-1a2f70ad0fd3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We save all the previous results in the `overal_result` dictionary and save it in `overal_result.json` to be loaded for table creation."
   ],
   "id": "4fadc0ee-8946-4021-b3d0-2d7079ae21f6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outputs in a json file\n",
    "with open(\"experiments/exp2/overal_result.json\", \"w\") as f:\n",
    "    json.dump(overal_result, f)"
   ],
   "id": "35005fc5-2ae2-4fd2-afec-3af0b57a51fe"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "The table is created in the next cell so we can compare our results with the table from second claims."
   ],
   "id": "3410dd8f-ff79-4060-8b5e-d0a21cff6f68"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from json file\n",
    "with open(\"experiments/exp2/overal_result.json\", \"r\") as f:\n",
    "    overal_result = json.load(f)\n",
    "\n",
    "# Create a dataframe with the result to be in a table form\n",
    "df = pd.DataFrame.from_dict(overal_result).rename(index={0: \"Random Init\", 1: \"Warm Start\", 2: \"Difference\"})\n",
    "\n",
    "# Display the dataframe\n",
    "display(df.style.set_properties(**{'text-align': 'center', 'border': '1px solid black', 'padding': '5px'}))"
   ],
   "id": "b52c9353-0af8-443f-8a51-0e3994d19acc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "<p style=\"color: crimson;font-size: 16px;\">\n",
    "Did the experiment description provide all the parameter values or did we make any assumptions? If so, what criteria do you think was used to make those assumptions?\n",
    "</p>"
   ],
   "id": "bf6ee792-429d-4216-b096-60b0fe89b593"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: green; font-size: 16px;\">\n",
    "Answer:\n",
    "</p>\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "20cb31f7-a276-4b8d-b2cf-9aa1bec18834"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to try:\n",
    "\n",
    "In this experiment you can:\n",
    "\n",
    "-   Change the learning rate by setting `lr=0.0001` as an argument in the `train_to_threshold` function\n",
    "-   Experiment with different `train_threshold` values and see how they affect the training time and the generalization gap\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "103c2d23-5d60-499f-a7fc-ef46bf4cba72"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using colab click on this link to go to the next notebook: [Open in Colab](https://colab.research.google.com/github/mohammed183/ml-reproducibility-p1/blob/main/notebooks/05-Experiment3.ipynb)"
   ],
   "id": "58f90f8b-124c-4308-8147-f273c21e6dad"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
