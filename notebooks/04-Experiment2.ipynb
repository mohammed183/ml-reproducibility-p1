{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2:\n",
    "\n",
    "In this experiment, we compare two methods of weight initialization: warm-starting and random initialization, for two models: **ResNet18** and **3-layer MLP** with tanh activation. We also compare two optimizers: **SGD** and **Adam**, for updating the weights based on the gradients. We use three image classification datasets: **CIFAR-10**, **CIFAR-100** and **SVHN**, and report the test accuracy of each model on each dataset. All models are trained using a mini-batch size of 128 and a learning rate of 0.001.\n",
    "\n",
    "We use the same components as in experiment one: the `get_loaders` function to get the required datasetâ€™s train and test loaders, the [**ResNet18**](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html) model from `torchvision.models`, and the [**SGD**](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) optimizer from `torch.optim`. We also introduce some new components:\n",
    "\n",
    "1.  The **CIFAR-100** dataset, which has 60,000 color images of 100 classes. To get the CIFAR-100 dataset, we pass the string `cifar100` as the dataset name argument to the `get_loaders` function.\n",
    "2.  The **SVHN** dataset, which has 73,257 color images of 10 classes of street view house numbers. To get the SVHN dataset, we pass the string `svhn` as the dataset name argument to the `get_loaders` function.\n",
    "3.  The `MLP` class that defines a 3-layer MLP model with a tanh activation function and a bias term.\n",
    "4.  The `torch.optim.Adam` class that implements the [**Adam**](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) optimizer, which is an adaptive learning rate method.\n",
    "5.  The `train_to_threshold` function that trains the model until it reaches 99% training accuracy, following the original paper."
   ],
   "id": "34729479-7d0f-4731-b7e2-211f65733e46"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We import the required packages as before."
   ],
   "id": "bd38d411-bd77-4462-88a1-081a6ed30777"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import transforms, datasets, models"
   ],
   "id": "ef252d4d-b139-41cf-8118-2aa79e4a9340"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "This is the same `get_loaders` function from Experiment 1"
   ],
   "id": "2573096e-cc06-4a01-a6e6-4ba3fbd452dd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(dataset=\"cifar10\", use_half_train=False, batch_size=128, dataset_portion=None):\n",
    "    \"\"\"\n",
    "    This loads the whole CIFAR-10 into memory and returns train and test data according to params\n",
    "    @param use_half_train (bool): return half the data or the whole train data\n",
    "    @param batch_size (int): batch size for training and testing\n",
    "    @param dataset_portion (double): portion of train data\n",
    "\n",
    "    @returns dict() with train and test data loaders with keys `train_loader`, `test_loader`\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalization using channel means\n",
    "    normalize_transform = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "    # Creating transform function\n",
    "    train_transform = transforms.Compose([transforms.ToTensor(), normalize_transform])\n",
    "        \n",
    "    # Test transformation function    \n",
    "    test_transform = transforms.Compose([transforms.ToTensor(), normalize_transform])\n",
    "    \n",
    "    # Check which dataset is required and load data from torchvision datasets\n",
    "    if dataset == 'cifar10':\n",
    "        original_train_dataset = datasets.CIFAR10(root=os.path.join('data', 'cifar10_data'),\n",
    "                                             train=True, transform=train_transform, download=True)\n",
    "        original_test_dataset = datasets.CIFAR10(root=os.path.join('data', 'cifar10_data'),\n",
    "                                             train=False, transform=test_transform, download=True)\n",
    "    elif dataset == 'cifar100':\n",
    "        original_train_dataset = datasets.CIFAR100(root=os.path.join('data', 'cifar100_data'),\n",
    "                                             train=True, transform=train_transform, download=True)\n",
    "        original_test_dataset = datasets.CIFAR100(root=os.path.join('data', 'cifar100_data'),\n",
    "                                             train=False, transform=test_transform, download=True)\n",
    "    else:\n",
    "        original_train_dataset = datasets.SVHN(root=os.path.join('data', 'svhn_data'),\n",
    "                                             split='train', transform=train_transform, download=True)\n",
    "        original_test_dataset = datasets.SVHN(root=os.path.join('data', 'svhn_data'),\n",
    "                                             split='test', transform=test_transform, download=True)\n",
    "    \n",
    "    # Check half data flag\n",
    "    if use_half_train:\n",
    "        print('Using Half Data')\n",
    "        dataset_portion = 0.5\n",
    "        \n",
    "    # Check if only a portion is required\n",
    "    if dataset_portion:\n",
    "        dataset_size = len(original_train_dataset)\n",
    "        split = int(np.floor((1 - dataset_portion) * dataset_size))\n",
    "        original_train_dataset, _ = random_split(original_train_dataset, [dataset_size - split, split])\n",
    "    \n",
    "    # Creating data loaders\n",
    "    loader_args = {\n",
    "        \"batch_size\": batch_size,\n",
    "    }\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=original_train_dataset,\n",
    "        shuffle=True,\n",
    "        **loader_args)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=original_test_dataset,\n",
    "        shuffle=False,\n",
    "        **loader_args)\n",
    "\n",
    "    return {\"train_loader\": train_loader,\n",
    "            \"test_loader\": test_loader}"
   ],
   "id": "e7f7c516-3eb3-42ed-8c55-762ce0f8f51a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "The following is a class for a **multilayer perceptron** (MLP) model with several fully connected (fc) layers and a final fully connected layer for the logits output. The arguments are:\n",
    "\n",
    "-   `input_dim`: the input feature dimension.\n",
    "-   `num_classes`: the output class number.\n",
    "-   `hidden_units`: the hidden unit number for each fc layer we set the default as 100 dimension as mentioned in the appendix.\n",
    "-   `activation`: the activation function, either `tanh` or `relu`.\n",
    "-   `bias`: whether to use bias terms in the fc layers.\n",
    "\n",
    "The function returns an MLP model object that can be trained or tested. The forward method takes an input tensor x and returns an output tensor x with the logits values. The output tensor does not have a final activation function. This will be used to create the **3-layer MLP** model."
   ],
   "id": "15bd26a5-b8ee-4cd4-846d-23ca646e4d34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class for a multilayer perceptron (MLP) model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=10, hidden_units=[100, 100, 100], activation='tanh', bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Check that the activation argument is valid\n",
    "        assert activation in ['tanh', 'relu'], \"Activation must be tanh or relu\"\n",
    "\n",
    "        # Assign the activation function based on the argument\n",
    "        if activation == 'tanh':\n",
    "            self.activation_function = torch.tanh\n",
    "        if activation == 'relu':\n",
    "            self.activation_function = torch.relu\n",
    "        \n",
    "        # Store num_classes and input_dim to be used in forward function\n",
    "        self.num_classes = num_classes\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # Initialize a variable to keep track of the last dimension of the layers\n",
    "        last_dim = input_dim\n",
    "        \n",
    "        # Initialize an empty list to store the fully connected (fc) layers\n",
    "        self.fcs = []\n",
    "        \n",
    "        # Loop through the hidden units argument and create fc layers with the given dimensions and bias\n",
    "        for i, n_h in enumerate(hidden_units):\n",
    "            self.fcs.append(nn.Linear(last_dim, n_h, bias=bias))\n",
    "            # Register the fc layer as a submodule with a name\n",
    "            self.add_module(f\"hidden_layer_{i}\", self.fcs[-1])\n",
    "            # Update the last dimension to match the output dimension of the fc layer\n",
    "            last_dim = n_h\n",
    "            \n",
    "        # Create a final fc layer for the logits output with the number of classes and bias\n",
    "        self.logit_fc = nn.Linear(last_dim, self.num_classes, bias=bias)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Reshape the input x to have a batch size and an input dimension\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        \n",
    "        # Loop through the fc layers and apply them to x with the activation function\n",
    "        for fc in self.fcs:\n",
    "            x = fc(x)\n",
    "            x = self.activation_function(x)\n",
    "            \n",
    "        # Apply the final fc layer to x and return it as the output\n",
    "        x = self.logit_fc(x)\n",
    "        \n",
    "        # x is returned without adding the final activation\n",
    "        return x"
   ],
   "id": "33e885fc-bb09-49c5-993a-087ded354d30"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "The following cell defines two functions that perform one epoch of training or evaluation on a data loader. They return the average loss and accuracy of the model. These functions will be used in the training function to run epoch by epoch."
   ],
   "id": "42a13590-129e-4ac7-83dd-d779aab6491a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function takes predictions and true values to return accuracies\n",
    "def get_accuracy(logit, true_y):\n",
    "    pred_y = torch.argmax(logit, dim=1)\n",
    "    return (pred_y == true_y).float().mean()\n",
    "    \n",
    "def eval_on_dataloader(device, criterion, model, dataloader):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a given data loader and return the average loss and accuracy.\n",
    "\n",
    "    Parameters:\n",
    "    device: the device (cpu or gpu) to use for computation\n",
    "    criterion: the loss function to use\n",
    "    model: the model to evaluate\n",
    "    dataloader: the data loader to iterate over the data\n",
    "\n",
    "    Returns:\n",
    "    loss: the average loss over the data loader\n",
    "    accuracy: the average accuracy over the data loader\n",
    "    \"\"\"\n",
    "    # Lists to store accuracy and loss\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    \n",
    "    for batch_idx, (data_x, data_y) in enumerate(dataloader): \n",
    "        data_x = data_x.to(device) \n",
    "        data_y = data_y.to(device)\n",
    "        \n",
    "        # get the model output for the input data\n",
    "        model_y = model(data_x) \n",
    "        \n",
    "        # compute the loss and accuracy\n",
    "        loss = criterion(model_y, data_y)\n",
    "        batch_accuracy = get_accuracy(model_y, data_y)\n",
    "        \n",
    "        # append accuracy and loss to lists\n",
    "        accuracies.append(batch_accuracy.item()) \n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # compute average loss and accuracy\n",
    "    loss = np.mean(losses) \n",
    "    accuracy = np.mean(accuracies) \n",
    "    return loss, accuracy \n",
    "\n",
    "\n",
    "def train_one_epoch(device, model, optimizer, criterion, dataloader):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch on a given training data loader and return the average loss and accuracy.\n",
    "\n",
    "    Parameters:\n",
    "    device: the device (cpu or gpu) to use for computation\n",
    "    model: the model to train\n",
    "    optimizer: the optimizer to use for updating the weights\n",
    "    criterion: the loss function to use\n",
    "    train_dataloader: the training data loader to iterate over the training data\n",
    "\n",
    "    Returns:\n",
    "    train_loss: the average loss over the training data loader\n",
    "    train_accuracy: the average accuracy over the training data loader\n",
    "    \"\"\"\n",
    "    # Lists to store accuracy and loss\n",
    "    accuracies = []\n",
    "    losses = [] \n",
    "    \n",
    "    for batch_idx, (data_x, data_y) in enumerate(dataloader):\n",
    "        data_x = data_x.to(device) \n",
    "        data_y = data_y.to(device) \n",
    "        \n",
    "         # reset the gradients of the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # get the model output for the input data\n",
    "        model_y = model(data_x)\n",
    "        \n",
    "        # compute the loss and accuracy\n",
    "        loss = criterion(model_y, data_y)\n",
    "        batch_accuracy = get_accuracy(model_y, data_y)\n",
    "        \n",
    "        # compute the gradients and update model parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # append accuracy and loss to lists\n",
    "        accuracies.append(batch_accuracy.item()) \n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # compute average loss and accuracy\n",
    "    loss = np.mean(losses) \n",
    "    accuracy = np.mean(accuracies) \n",
    "    return loss, accuracy \n"
   ],
   "id": "4628c251-cd88-4bff-83dc-270eebe7b5e0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "The `train_to_threshold` function is the same as `train_model_exp1` functions except that it trains the model until a certain `training_threshold` or until the change in the training accuracy doesnâ€™t exceed `convergence_change_threshold` for certain number of epochs. The new parameter introduced are:\n",
    "\n",
    "-   `train_threshold`: The training accuracy at which the model stops training.\n",
    "-   `convergence_change_threshold`: The minimum accuracy change to continue training.\n",
    "-   `convergence_epochs`: The maximum number of epochs allowed with insufficient change in training accuracy before stopping the training."
   ],
   "id": "d746fc19-508f-438a-b46f-05f065b68680"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_to_threshold(title='warm', lr=0.001, checkpoint=None, use_half_data=False, convergence_epochs=4,\n",
    "                       train_threshold=0.5, convergence_change_threshold=0.002, random_seed=42):\n",
    "    # use gpu if available ( change device id if needed )\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "        print(\"CUDA Recognized\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Get the dataset\n",
    "    loaders = get_loaders(dataset=\"cifar10\", use_half_train=use_half_data)\n",
    "\n",
    "    # Get the model\n",
    "    model = models.resnet18(num_classes=10).to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    # Create the loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Get model from checkpoint\n",
    "    if checkpoint is not None:\n",
    "        model.load_state_dict(torch.load(checkpoint, map_location=device)['model'])\n",
    "\n",
    "    print(f\"Training {'warm-starting' if checkpoint is not None else 'random initialized'} \" \\\n",
    "        f\"ResNet-18 model with SGD optimizer on {'50%' if use_half_data else '100%'} of cifar10 dataset\")\n",
    "\n",
    "    # initialize training varaibles\n",
    "    train_accuracies = []\n",
    "    stop_indicator = False\n",
    "    model_name = 'resnet18'\n",
    "    epoch = 0\n",
    "    # Train until convergence or stop condition is met\n",
    "    while(not stop_indicator):\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"\\t Training in epoch {epoch + 1} \\t\")\n",
    "        # Train for one epoch and get loss and accuracy\n",
    "        train_loss, train_accuracy = train_one_epoch(device, model, optimizer, criterion, loaders['train_loader'])\n",
    "\n",
    "        # Append training accuracy to list\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        epoch += 1\n",
    "        # Check if training accuracy is above a threshold\n",
    "        if train_accuracy >= train_threshold:\n",
    "            print(f\"Convergence codition met. Training accuracy > {train_threshold}\")\n",
    "            stop_indicator = True\n",
    "\n",
    "        # Check if training accuracy has stopped improving for a number of epochs\n",
    "        if len(train_accuracies) >= convergence_epochs:\n",
    "            if np.std(train_accuracies[-convergence_epochs:]) < convergence_change_threshold:\n",
    "                print(f\"\\tConvergence codition met. Training accuracy = {train_accuracy} stopped improving\")\n",
    "                stop_indicator = True\n",
    "\n",
    "    # Evaluate on test set and get loss and accuracy\n",
    "    test_loss, test_accuracy =  eval_on_dataloader(device, criterion, model, loaders['test_loader'])\n",
    "    print(f\"\\tTest accuracy = {test_accuracy}\")\n",
    "\n",
    "    # Save the model if will be used for warm-starting\n",
    "    if use_half_data:\n",
    "        # Create directory in exp with experiment title\n",
    "        experiment_dir = os.path.join('experiments/exp2', title)\n",
    "        os.makedirs(experiment_dir, exist_ok=True)\n",
    "        \n",
    "        # save the model\n",
    "        model_name = model_name+'-sgd'\n",
    "        model_directory =  os.path.join(experiment_dir, f'{model_name}.pt')\n",
    "        torch.save({\n",
    "            'model': model.state_dict()\n",
    "        }, model_directory)\n",
    "\n",
    "        print(f\"Model saved to checkpoint: {model_directory}\")\n",
    "\n",
    "    return test_accuracy * 100 "
   ],
   "id": "1a142472-5e36-4297-8775-21f5929dd351"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "The ResNet-18 model is trained on the CIFAR-10 dataset using an SGD optimizer and the `train_to_threshold` function. It is trained with warm-start and random initialization."
   ],
   "id": "2556627b-e489-4c49-9060-195f280fc1c4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to save all results\n",
    "overal_result = {}\n",
    "\n",
    "# train on full data with random initialization\n",
    "random_init = train_to_threshold(title='resnet-sgd-cifar10', train_threshold=0.99)"
   ],
   "id": "cf0b055f-b4bd-4ac0-aa0c-5f175d62b075"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on half data\n",
    "_ = train_to_threshold(title='resnet-sgd-cifar10', train_threshold=0.99, use_half_data=True)\n",
    "\n",
    "# train on full data with warm-starting\n",
    "warm_start = train_to_threshold(title='resnet-sgd-cifar10', train_threshold=0.99,\n",
    "                                     checkpoint='experiments/exp2/resnet-sgd-cifar10/resnet18-sgd.pt')"
   ],
   "id": "bae226df-c63a-4b83-b285-9e894fa52cd3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the difference between random and warm-start models using sgd optimizer on CIFAR-10\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overal_result['resnet-sgd-cifar10'] = [random_init, warm_start, diff]"
   ],
   "id": "b53a028f-551a-490e-b83a-926caa4141be"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We extend this experiment by training the same model with the Adam optimizer instead of SGD. We add a new parameter `optimizer_name` to select the optimizer for the model."
   ],
   "id": "d3494c23-2d39-4efa-8f44-25920660ea18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_to_threshold(title='warm', lr=0.001, checkpoint=None, \n",
    "                       use_half_data=False, optimizer_name='adam', convergence_epochs=4,\n",
    "                       train_threshold=0.5, convergence_change_threshold=0.002, random_seed=42):\n",
    "    # use gpu if available ( change device id if needed )\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "        print(\"CUDA Recognized\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Get the dataset\n",
    "    loaders = get_loaders(dataset=\"cifar10\", use_half_train=use_half_data)\n",
    "\n",
    "    # Get the model\n",
    "    model = models.resnet18(num_classes=10).to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    if optimizer_name == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Create the loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Get model from checkpoint\n",
    "    if checkpoint is not None:\n",
    "        model.load_state_dict(torch.load(checkpoint, map_location=device)['model'])\n",
    "\n",
    "    print(f\"Training {'warm-starting' if checkpoint is not None else 'random initialized'} ResNet-18 model \" \\\n",
    "            f\"with {optimizer_name.upper()} optimizer on {'50%' if use_half_data else '100%'} of cifar10 dataset\")\n",
    "\n",
    "    # initialize training varaibles\n",
    "    train_accuracies = []\n",
    "    stop_indicator = False\n",
    "    model_name = 'resnet18'\n",
    "    epoch = 0\n",
    "    # Train until convergence or stop condition is met\n",
    "    while(not stop_indicator):\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"\\t Training in epoch {epoch + 1} \\t\")\n",
    "        # Train for one epoch and get loss and accuracy\n",
    "        train_loss, train_accuracy = train_one_epoch(device, model, optimizer, criterion, loaders['train_loader'])\n",
    "\n",
    "        # Append training accuracy to list\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        epoch += 1\n",
    "        # Check if training accuracy is above a threshold\n",
    "        if train_accuracy >= train_threshold:\n",
    "            print(f\"Convergence codition met. Training accuracy > {train_threshold}\")\n",
    "            stop_indicator = True\n",
    "\n",
    "        # Check if training accuracy has stopped improving for a number of epochs\n",
    "        if len(train_accuracies) >= convergence_epochs:\n",
    "            if np.std(train_accuracies[-convergence_epochs:]) < convergence_change_threshold:\n",
    "                print(f\"\\tConvergence codition met. Training accuracy = {train_accuracy} stopped improving\")\n",
    "                stop_indicator = True\n",
    "\n",
    "    # Evaluate on test set and get loss and accuracy\n",
    "    test_loss, test_accuracy =  eval_on_dataloader(device, criterion, model, loaders['test_loader'])\n",
    "    print(f\"\\tTest accuracy = {test_accuracy}\")\n",
    "\n",
    "    # Save the model if will be used for warm-starting\n",
    "    if use_half_data:\n",
    "        # Create directory in exp with experiment title\n",
    "        experiment_dir = os.path.join('experiments/exp2', title)\n",
    "        os.makedirs(experiment_dir, exist_ok=True)\n",
    "        \n",
    "        # save the model\n",
    "        model_name = model_name+'-'+optimizer_name\n",
    "        model_directory =  os.path.join(experiment_dir, f'{model_name}.pt')\n",
    "        torch.save({\n",
    "            'model': model.state_dict()\n",
    "        }, model_directory)\n",
    "\n",
    "        print(f\"Model saved to checkpoint: {model_directory}\")\n",
    "\n",
    "    return test_accuracy * 100"
   ],
   "id": "c741860f-8072-47bb-b931-c2accefd0a97"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We repeat the training of the same models with the Adam optimizer instead of SGD."
   ],
   "id": "b66c67b5-1fd4-41b4-bbb9-8b27d32b4b6e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on full data with random initialization but with Adam\n",
    "random_init = train_to_threshold(title='resnet-adam-cifar10', train_threshold=0.99, optimizer_name='adam')\n",
    "\n",
    "# train on half data\n",
    "_ = train_to_threshold(title='resnet-adam-cifar10', train_threshold=0.99,\n",
    "                                             optimizer_name='adam', use_half_data=True)\n",
    "\n",
    "# train on full data with warm-starting but with Adam\n",
    "warm_start = train_to_threshold(title='resnet-adam-cifar10', train_threshold=0.99, optimizer_name='adam',\n",
    "                                        checkpoint='experiments/exp2/resnet-adam-cifar10/resnet18-adam.pt')\n",
    "\n",
    "# get the difference between random and warm-start models using Adam optimizer on CIFAR-10\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overal_result['resnet-adam-cifar10'] = [random_init, warm_start, diff]"
   ],
   "id": "77fe42f9-0b50-4f7a-b403-9ac544550b8a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We also experiment with the MLP model instead of the ResNet. We introduce a parameter `model_name` to select the model for the training."
   ],
   "id": "ea3e6063-8c24-4154-8fd4-ed04ff1e7bfe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_to_threshold(title='warm', lr=0.001, checkpoint=None, use_half_data=False,\n",
    "                        optimizer_name='adam', model_name='resnet18', convergence_epochs=4,\n",
    "                        train_threshold=0.5, convergence_change_threshold=0.002, random_seed=42):\n",
    "    # use gpu if available ( change device id if needed )\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "        print(\"CUDA Recognized\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Get the dataset\n",
    "    loaders = get_loaders(dataset=\"cifar10\", use_half_train=use_half_data)\n",
    "\n",
    "    # Get the model\n",
    "    if model_name == 'resnet18':\n",
    "        model = models.resnet18(num_classes=10).to(device)\n",
    "    else:\n",
    "        model = MLP( input_dim = 32 * 32 * 3, num_classes=10).to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    if optimizer_name == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Create the loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Get model from checkpoint\n",
    "    if checkpoint is not None:\n",
    "        model.load_state_dict(torch.load(checkpoint, map_location=device)['model'])\n",
    "\n",
    "    print(f\"Training {'warm-starting' if checkpoint is not None else 'random initialized'} {model_name} model \" \\\n",
    "            f\"with {optimizer_name.upper()} optimizer on {'50%' if use_half_data else '100%'} of cifar10 dataset\")\n",
    "\n",
    "    # initialize training varaibles\n",
    "    train_accuracies = []\n",
    "    stop_indicator = False\n",
    "    epoch = 0\n",
    "    # Train until convergence or stop condition is met\n",
    "    while(not stop_indicator):\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"\\t Training in epoch {epoch + 1} \\t\")\n",
    "        # Train for one epoch and get loss and accuracy\n",
    "        train_loss, train_accuracy = train_one_epoch(device, model, optimizer, criterion, loaders['train_loader'])\n",
    "\n",
    "        # Append training accuracy to list\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        epoch += 1\n",
    "        # Check if training accuracy is above a threshold\n",
    "        if train_accuracy >= train_threshold:\n",
    "            print(f\"Convergence codition met. Training accuracy > {train_threshold}\")\n",
    "            stop_indicator = True\n",
    "\n",
    "        # Check if training accuracy has stopped improving for a number of epochs\n",
    "        if len(train_accuracies) >= convergence_epochs:\n",
    "            if np.std(train_accuracies[-convergence_epochs:]) < convergence_change_threshold:\n",
    "                print(f\"\\tConvergence codition met. Training accuracy = {train_accuracy} stopped improving\")\n",
    "                stop_indicator = True\n",
    "\n",
    "    # Evaluate on test set and get loss and accuracy\n",
    "    test_loss, test_accuracy =  eval_on_dataloader(device, criterion, model, loaders['test_loader'])\n",
    "    print(f\"\\tTest accuracy = {test_accuracy}\")\n",
    "\n",
    "    # Save model if it is needed for warm-starting\n",
    "    if use_half_data:\n",
    "        # Create directory in exp with experiment title\n",
    "        experiment_dir = os.path.join('experiments/exp2', title)\n",
    "        os.makedirs(experiment_dir, exist_ok=True)\n",
    "        \n",
    "        # save the model\n",
    "        model_name = model_name+'-'+optimizer_name\n",
    "        model_directory =  os.path.join(experiment_dir, f'{model_name}.pt')\n",
    "        torch.save({\n",
    "            'model': model.state_dict()\n",
    "        }, model_directory)\n",
    "\n",
    "        print(f\"Model saved to checkpoint: {model_directory}\")\n",
    "\n",
    "    return test_accuracy * 100"
   ],
   "id": "09ec77af-414c-47f8-b835-a9605c2b64e4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We use warm-starting and the SGD optimizer to train the MLP model in the next cell."
   ],
   "id": "996f5f1a-5491-459e-a866-4024651238e3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train MLP model on full data with random initialization and SGD optimizer\n",
    "random_init = train_to_threshold(title='mlp-sgd-cifar10', train_threshold=0.99, \n",
    "                                 optimizer_name='sgd', model_name='mlp')\n",
    "\n",
    "# train MLP model on half data\n",
    "_ = train_to_threshold(title='mlp-sgd-cifar10', train_threshold=0.99, \n",
    "                       optimizer_name='sgd', model_name='mlp', use_half_data=True)\n",
    "\n",
    "# train MLP on full data with warm-starting and SGD optimizer\n",
    "warm_start = train_to_threshold(title='mlp-sgd-cifar10', train_threshold=0.99, optimizer_name='sgd', \n",
    "                                model_name='mlp', checkpoint='experiments/exp2/mlp-sgd-cifar10/mlp-sgd.pt')\n",
    "\n",
    "# get the difference between random and warm-start MLP models using SGD optimizer on CIFAR-10\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overal_result['mlp-sgd-cifar10'] = [random_init, warm_start, diff]"
   ],
   "id": "c59666e9-a27c-4218-b337-ec2d8a0fa5e8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "In the next cell, we train the MLP model with warm-starting and the Adam optimizer."
   ],
   "id": "7fd563b5-d053-44d5-9769-848b154416b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train MLP model on full data with random initialization\n",
    "random_init= train_to_threshold(title='mlp-adam-cifar10', train_threshold=0.99, \n",
    "                                optimizer_name='adam', model_name='mlp')\n",
    "\n",
    "# train MLP mode on half data\n",
    "_ = train_to_threshold(title='mlp-adam-cifar10', train_threshold=0.99, \n",
    "                       optimizer_name='adam', model_name='mlp', use_half_data=True)\n",
    "\n",
    "# train MLP on full data with warm-starting\n",
    "warm_start = train_to_threshold(title='mlp-adam-cifar10', train_threshold=0.99, optimizer_name='adam',\n",
    "                                model_name='mlp', checkpoint='experiments/exp2/mlp-adam-cifar10/mlp-adam.pt')\n",
    "\n",
    "# get the difference between random and warm-start MLP models using Adam optimizer on CIFAR-10\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overal_result['mlp-adam-cifar10'] = [random_init, warm_start, diff]"
   ],
   "id": "312d844d-9d5f-4da3-8193-0168d043507c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "Finaly we extend this to allow using the SVHN and CIFAR-100 datasets by adding a parameter `dataset` to specify the dataset we would like to use."
   ],
   "id": "0c88ef01-d6b4-4601-8dcc-a1828c2b5258"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_to_threshold(title='warm', dataset='cifar10', lr=0.001, checkpoint=None, use_half_data=False,\n",
    "                       optimizer_name='adam', model_name='resnet18', convergence_epochs=4,\n",
    "                       train_threshold=0.5, convergence_change_threshold=0.002, random_seed=42):\n",
    "    # use gpu if available ( change device id if needed )\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "        print(\"CUDA Recognized\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Get the dataset\n",
    "    loaders = get_loaders(dataset=dataset, use_half_train=use_half_data)\n",
    "\n",
    "    # Define the number of classes\n",
    "    num_classes = 10\n",
    "    if dataset == 'cifar100':\n",
    "        num_classes=100\n",
    "\n",
    "    # Get the model\n",
    "    if model_name == 'resnet18':\n",
    "        model = models.resnet18(num_classes=num_classes).to(device)\n",
    "    else:\n",
    "        model = MLP( input_dim = 32 * 32 * 3, num_classes=num_classes).to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    if optimizer_name == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Create the loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Get model from checkpoint\n",
    "    if checkpoint is not None:\n",
    "        model.load_state_dict(torch.load(checkpoint, map_location=device)['model'])\n",
    "\n",
    "    print(f\"Training {'random initialized' if checkpoint is None else 'warm-starting'} {model_name} model wtih \" \\\n",
    "            f\"{optimizer_name.upper()} optimizer on {'50%' if use_half_data else '100%'} of {dataset} dataset\")\n",
    "\n",
    "    # initialize training variables\n",
    "    train_accuracies = []\n",
    "    stop_indicator = False\n",
    "    epoch = 0\n",
    "    # Train until convergence or stop condition is met\n",
    "    while(not stop_indicator):\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"\\t Training in epoch {epoch + 1} \\t\")\n",
    "        # Train for one epoch and get loss and accuracy\n",
    "        train_loss, train_accuracy = train_one_epoch(device, model, optimizer, criterion, loaders['train_loader'])\n",
    "\n",
    "        # Append training accuracy to list\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        epoch += 1\n",
    "        # Check if training accuracy is above a threshold\n",
    "        if train_accuracy >= train_threshold:\n",
    "            print(f\"Convergence codition met. Training accuracy > {train_threshold}\")\n",
    "            stop_indicator = True\n",
    "\n",
    "        # Check if training accuracy has stopped improving for a number of epochs\n",
    "        if len(train_accuracies) >= convergence_epochs:\n",
    "            if np.std(train_accuracies[-convergence_epochs:]) < convergence_change_threshold:\n",
    "                print(f\"\\tConvergence codition met. Training accuracy = {train_accuracy} stopped improving\")\n",
    "                stop_indicator = True\n",
    "\n",
    "    # Evaluate on test set and get loss and accuracy\n",
    "    test_loss, test_accuracy =  eval_on_dataloader(device, criterion, model, loaders['test_loader'])\n",
    "    print(f\"\\tTest accuracy = {test_accuracy}\")\n",
    "\n",
    "    # Save the model if it will be used for warm-starting\n",
    "    if use_half_data:\n",
    "        # Create directory in exp with experiment title\n",
    "        experiment_dir = os.path.join('experiments/exp2', title)\n",
    "        os.makedirs(experiment_dir, exist_ok=True)\n",
    "\n",
    "        # save the model\n",
    "        model_name = model_name+'-'+optimizer_name\n",
    "        model_directory =  os.path.join(experiment_dir, f'{model_name}.pt')\n",
    "        torch.save({\n",
    "            'model': model.state_dict()\n",
    "        }, model_directory)\n",
    "\n",
    "        print(f\"Model saved to checkpoint: {model_directory}\")\n",
    "\n",
    "    return test_accuracy * 100"
   ],
   "id": "aa0dbf69-8588-4fed-9c5d-837dec8c049a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We repeat all the previous for the CIFAR-100 dataset using the ResNet model with different optimizers."
   ],
   "id": "c4700153-ddd6-4bca-a142-6e158ff862b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Resnet model on full CIFAR-100 data with random initialization and SGD optimizer\n",
    "random_init = train_to_threshold(title='resnet-sgd-cifar100', dataset='cifar100', train_threshold=0.99,\n",
    "                                       optimizer_name='sgd', model_name='resnet18')\n",
    "\n",
    "# train on half CIFAR-100 data\n",
    "_ = train_to_threshold(title='resnet-sgd-cifar100', dataset='cifar100', train_threshold=0.99, optimizer_name='sgd',\n",
    "                   model_name='resnet18', use_half_data=True)\n",
    "\n",
    "# train Resnet model on full CIFAR-100 data with warm-starting and SGD optimizer\n",
    "warm_start = train_to_threshold(title='resnet-sgd-cifar100', dataset='cifar100', train_threshold=0.99, \n",
    "                                optimizer_name='sgd', model_name='resnet18', \n",
    "                                checkpoint='experiments/exp2/resnet-sgd-cifar100/resnet18-sgd.pt')\n",
    "\n",
    "# get the difference between random and warm-start ResNet models using SGD optimizer on CIFAR-100\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overal_result['resnet-sgd-cifar100'] = [random_init, warm_start, diff]\n",
    "\n",
    "# train Resnet model on full CIFAR-100 data with random initialization and Adam optimizer\n",
    "random_init = train_to_threshold(title='resnet-adam-cifar100', dataset='cifar100', train_threshold=0.99,\n",
    "                                            optimizer_name='adam', model_name='resnet18')\n",
    "\n",
    "# train on half CIFAR-100 data\n",
    "_ = train_to_threshold(title='resnet-adam-cifar100', dataset='cifar100', train_threshold=0.99, \n",
    "                       optimizer_name='adam', model_name='resnet18', use_half_data=True)\n",
    "\n",
    "# train Resnet model on full CIFAR-100 data with warm-starting and Adam optimizer\n",
    "warm_start = train_to_threshold(title='resnet-adam-cifar100', dataset='cifar100', train_threshold=0.99, \n",
    "                                optimizer_name='adam', model_name='resnet18', \n",
    "                                checkpoint='experiments/exp2/resnet-adam-cifar100/resnet18-adam.pt')\n",
    "\n",
    "# get the difference between random and warm-start ResNet models using Adam optimizer on CIFAR-100\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overal_result['resnet-adam-cifar100'] = [random_init, warm_start, diff]"
   ],
   "id": "b6236a25-6e79-41f8-99e0-6ea2bc66a8f1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We repeat all the previous for the CIFAR-100 dataset using the MLP model with different optimizers."
   ],
   "id": "ae350d06-5806-4386-bb75-ea88fa0f976f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train MLP model on full CIFAR-100 data with random initialization and SGD optimizer\n",
    "random_init = train_to_threshold(title='mlp-sgd-cifar100', dataset='cifar100', train_threshold=0.99,\n",
    "                                             optimizer_name='sgd', model_name='mlp')\n",
    "\n",
    "# train on half CIFAR-100 data\n",
    "_ = train_to_threshold(title='mlp-sgd-cifar100', dataset='cifar100', train_threshold=0.99,\n",
    "                       optimizer_name='sgd', model_name='mlp', use_half_data=True)\n",
    "\n",
    "# train MLP model on full CIFAR-100 data with warm-starting and SGD optimizer\n",
    "warm_start = train_to_threshold(title='mlp-sgd-cifar100', dataset='cifar100', train_threshold=0.99, \n",
    "                                optimizer_name='sgd', model_name='mlp', \n",
    "                                checkpoint='experiments/exp2/mlp-sgd-cifar100/mlp-sgd.pt')\n",
    "\n",
    "# get the difference between random and warm-start MLP models using SGD optimizer on CIFAR-100\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overal_result['mlp-sgd-cifar100'] = [random_init, warm_start, diff]\n",
    "\n",
    "# train MLP model on full CIFAR-100 data with random initialization and Adam optimizer\n",
    "random_init = train_to_threshold(title='mlp-adam-cifar100', dataset='cifar100',train_threshold=0.99,\n",
    "                                              optimizer_name='adam', model_name='mlp')\n",
    "\n",
    "# train on half CIFAR-100 data\n",
    "_ = train_to_threshold(title='mlp-adam-cifar100', dataset='cifar100', train_threshold=0.99, \n",
    "                       optimizer_name='adam', model_name='mlp', use_half_data=True)\n",
    "\n",
    "# train MLP model on full CIFAR-100 data with warm-starting and Adam optimizer\n",
    "warm_start = train_to_threshold(title='mlp-adam-cifar100', dataset='cifar100', train_threshold=0.99, \n",
    "                                optimizer_name='adam', model_name='mlp', \n",
    "                                checkpoint='experiments/exp2/mlp-adam-cifar100/mlp-adam.pt')\n",
    "\n",
    "# get the difference between random and warm-start MLP models using Adam optimizer on CIFAR-100\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overal_result['mlp-adam-cifar100'] = [random_init, warm_start, diff]"
   ],
   "id": "0aff1743-1ea0-4b26-ab54-ddd0d91366fe"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We create the previous for the SVHN dataset using the ResNet model with different optimizers."
   ],
   "id": "056fbec0-71ac-4b34-a7a3-6f2fc8ca5362"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train ResNet model on full SVHN data with random initialization and SGD optimizer\n",
    "random_init = train_to_threshold(title='resnet-sgd-svhn', dataset='svhn', train_threshold=0.99,\n",
    "                                       optimizer_name='sgd', model_name='resnet18')\n",
    "\n",
    "# train on half SVHN data\n",
    "_ = train_to_threshold(title='resnet-sgd-svhn', dataset='svhn', train_threshold=0.99, \n",
    "                       optimizer_name='sgd', model_name='resnet18', use_half_data=True)\n",
    "\n",
    "# train ResNet model on full SVHN data with warm-starting and SGD optimizer\n",
    "warm_start = train_to_threshold(title='resnet-sgd-svhn', dataset='svhn', train_threshold=0.99, \n",
    "                                optimizer_name='sgd', model_name='resnet18',\n",
    "                                checkpoint='experiments/exp2/resnet-sgd-svhn/resnet18-sgd.pt')\n",
    "\n",
    "# store the difference between random and warm-start ResNet models using SGD optimizer on SVHN\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overal_result['resnet-sgd-svhn'] = [random_init, warm_start, diff]\n",
    "\n",
    "# train ResNet model on full SVHN data with random initialization and Adam optimizer\n",
    "random_init = train_to_threshold(title='resnet-adam-svhn', dataset='svhn', train_threshold=0.99,\n",
    "                                        optimizer_name='adam', model_name='resnet18')\n",
    "\n",
    "# train on half SVHN data\n",
    "_ = train_to_threshold(title='resnet-adam-svhn', dataset='svhn', train_threshold=0.99, \n",
    "                       optimizer_name='adam', model_name='resnet18', use_half_data=True)\n",
    "\n",
    "# train ResNet model on full SVHN data with warm-starting and Adam optimizer\n",
    "warm_start = train_to_threshold(title='resnet-adam-svhn', dataset='svhn', train_threshold=0.99, \n",
    "                                optimizer_name='adam', model_name='resnet18', \n",
    "                                checkpoint='experiments/exp2/resnet-adam-svhn/resnet18-adam.pt')\n",
    "\n",
    "# store the difference between random and warm-start ResNet models using Adam optimizer on SVHN\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overal_result['resnet-adam-svhn'] = [random_init, warm_start, diff]"
   ],
   "id": "96909b36-cf7e-4157-bbf6-187f4b0a9ad4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We create the previous for the SVHN dataset using the MLP model with different optimizers."
   ],
   "id": "70830ac4-fe71-4fe0-a720-1dab166d01e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train MLP model on full SVHN data with random initialization and SGD optimizer\n",
    "random_init = train_to_threshold(title='mlp-sgd-svhn', dataset='svhn', train_threshold=0.99,\n",
    "                                             optimizer_name='sgd', model_name='mlp')\n",
    "\n",
    "# train on half SVHN data\n",
    "_ = train_to_threshold(title='mlp-sgd-svhn', dataset='svhn', train_threshold=0.99, \n",
    "                       optimizer_name='sgd', model_name='mlp', use_half_data=True)\n",
    "\n",
    "# train MLP model on full SVHN data with warm-starting and SGD optimizer\n",
    "warm_start = train_to_threshold(title='mlp-sgd-svhn', dataset='svhn', train_threshold=0.99, \n",
    "                                optimizer_name='sgd', model_name='mlp',\n",
    "                                checkpoint='experiments/exp2/mlp-sgd-svhn/mlp-sgd.pt')\n",
    "\n",
    "# get the difference between random and warm-start MLP models using SGD optimizer on SVHN\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overal_result['mlp-sgd-svhn'] = [random_init, warm_start, diff]\n",
    "\n",
    "# train MLP model on full SVHN data with random initialization and Adam optimizer\n",
    "random_init = train_to_threshold(title='mlp-adam-svhn', dataset='svhn', train_threshold=0.99,\n",
    "                                              optimizer_name='adam', model_name='mlp')\n",
    "\n",
    "# train on half SVHN data\n",
    "_ = train_to_threshold(title='mlp-adam-svhn', dataset='svhn', train_threshold=0.99, \n",
    "                       optimizer_name='adam', model_name='mlp', use_half_data=True)\n",
    "\n",
    "# train MLP model on full SVHN data with warm-starting and Adam optimizer\n",
    "warm_start = train_to_threshold(title='mlp-adam-svhn', dataset='svhn', train_threshold=0.99, \n",
    "                                optimizer_name='adam', model_name='mlp',\n",
    "                                checkpoint='experiments/exp2/mlp-adam-svhn/mlp-adam.pt')\n",
    "\n",
    "# get the difference between random and warm-start MLP models using Adam optimizer on SVHN\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overal_result['mlp-adam-svhn'] = [random_init, warm_start, diff]"
   ],
   "id": "4ac28a4d-d4d7-4476-b9b4-9cc248f50bd8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We save all the previous results in the `overal_result` dictionary and save it in `overal_result.json` to be loaded for table creation."
   ],
   "id": "396c3565-2284-4a09-8958-dbd1ed12188e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outputs in a json file\n",
    "with open(\"experiments/exp2/overal_result.json\", \"w\") as f:\n",
    "    json.dump(overal_result, f)"
   ],
   "id": "e5ff60a9-7807-4a16-82d4-caed2c876140"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "The table is created in the next cell so we can compare our results with the table from second claims."
   ],
   "id": "2725da40-4e49-40a9-b16d-a1c95f9be685"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from json file\n",
    "with open(\"experiments/exp2/overal_result.json\", \"r\") as f:\n",
    "    overal_result = json.load(f)\n",
    "\n",
    "# Create a dataframe with the result to be in a table form\n",
    "df = pd.DataFrame.from_dict(overal_result).rename(index={0: \"Random Init\", 1: \"Warm Start\", 2: \"Difference\"})\n",
    "\n",
    "# Display the dataframe\n",
    "display(df.style.set_properties(**{'text-align': 'center', 'border': '1px solid black', 'padding': '5px'}))"
   ],
   "id": "ae6b238d-96a3-488d-93e2-9dc546968fc0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "<p style=\"color: crimson;font-size: 16px;\">\n",
    "Did the experiment description provide all the parameter values or did we make any assumptions? If so, what criteria do you think was used to make those assumptions?\n",
    "</p>"
   ],
   "id": "6f7c3866-8b32-4edf-8a67-c0148875ef15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: green; font-size: 16px;\">\n",
    "Answer:\n",
    "</p>\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "bc54f633-b331-470c-9be6-791c399b8a0f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to try:\n",
    "\n",
    "In this experiment you can:\n",
    "\n",
    "-   Change the learning rate by setting `lr=0.0001` as an argument in the `train_to_threshold` function\n",
    "-   Experiment with different `train_threshold` values and see how they affect the training time and the generalization gap\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "eb9229f9-6b8c-41bf-ba24-873e98e99d8d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using colab click on this link to go to the next notebook: [Open in Colab](https://colab.research.google.com/github/mohammed183/ml-reproducibility-p1/blob/main/notebooks/05-Experiment3.ipynb)"
   ],
   "id": "ba2470b0-660c-4fe2-a73f-1b508e12ef60"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
