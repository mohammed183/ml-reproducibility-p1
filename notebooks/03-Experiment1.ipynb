{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5775c558-49b1-4791-abc7-f5635c8782a0",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "In this section, we will test the claims made by the authors. You will come across sections in the code marked with `#TODO`, where you need to fill an argument as described in the experiment description.\n",
    "\n",
    "<p style=\"color: crimson;font-size: 16px;\">\n",
    "Note: The parameter values given in the paper are specified in the description of each experiment. Otherwise, we need to assume them.\n",
    "</p>\n",
    "\n",
    "------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25be3c1f-9087-4240-8f44-651832422bb5",
   "metadata": {},
   "source": [
    "## Experiment 1:\n",
    "\n",
    "In this experiment we want to compare two ways of training a ResNet-18 model, which is a type of deep neural network that can classify images. The CIFAR-10 dataset is a collection of 60,000 color images of 10 classes, such as airplanes, cars, and dogs. The experiment splits the dataset into two parts: a training set and a test set. The training set is used to update the model weights, and the test set is used to evaluate the model performance.\n",
    "\n",
    "The experiment uses two models: a warm-starting model and a randomly initialized model. The warm-starting model starts with some pre-trained weights that are learned training on 50% of the training data for 350 epochs. The randomly initialized model starts with random weights that are not learned from any data. Both models train on the full training data for 350 epochs, where one epoch means one pass over the entire data. The experiment will measure the accuracy of the models on both the training and test sets, which is the percentage of correctly classified images.\n",
    "\n",
    "To run this experiment we will need to:\n",
    "\n",
    "1.  Create `get_loaders` function to load the CIFAR-10 dataset and split it into training and test sets.\n",
    "2.  Define a function that takes a model, a data loader, an optimizer, and a loss function, and trains the model for a given number of epochs, saving the model weights after each epoch.\n",
    "3.  Create a ResNet-18 model and train it for 350 epochs on 50% of the training data, using stochastic gradient descent as the optimizer and cross entropy loss as the loss function. Save the final model weights as `half_cifar.pt`.\n",
    "4.  Create another ResNet-18 model and load the weights from `half_cifar.pt`. Train this model for another 350 epochs on the full training data, using the same optimizer and loss function. Save the final model weights as `warm_start_full.pt`.\n",
    "5.  Create a third ResNet-18 model with random weights. Train this model for 350 epochs on the full training data, using the same optimizer and loss function. Save the final model weights as `random_full.pt`.\n",
    "6.  Evaluate the test accuracy of all three models using the test data loader. Plot the accuracy curves of the models over time. Compare the results with those reported in the paper and analyze the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ad4732-50b1-413e-9287-40f9e41cba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import transforms, datasets, models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44665186-c3d3-471d-8d2d-eed14f201874",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "The following function is `get_loaders` which we use to load the CIFAR-10 dataset, which consists of 60,000 color images of 10 classes, and returns data loaders for training and testing. The function takes four parameters:\n",
    "\n",
    "-   `dataset`: A string that specifies the desired dataset. For this experiment, we use the `cifar10` key to access the CIFAR-10 dataset from the `dataset_factories` dictionary.\n",
    "-   `use_half_train`: a boolean flag that indicates whether to use only half of the training data or the whole dataset. If this is set to `True`, then the parameter `dataset_portion` is automatically set to 0.5.\n",
    "-   `batch_size`: an integer that specifies the number of images to process in each batch. A larger batch size may speed up the training but also require more memory.\n",
    "-   `dataset_portion`: a double value between 0 and 1 that indicates the portion of the training data to use. For example, if this is set to 0.8, then only 80% of the training data will be used and the rest will be discarded.\n",
    "\n",
    "The function returns a dictionary with two keys: `train_loader` and `test_loader` which can be used to iterate over the training and testing data respectively. The function also downloads the dataset from torchvision datasets if it is not already present in the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c70384e-8d54-4fa6-9131-ea88106bd3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(dataset=\"cifar10\", use_half_train=False, batch_size=128, dataset_portion=None):\n",
    "    \"\"\"\n",
    "    This loads the whole CIFAR-10 into memory and returns train and test data according to params\n",
    "    @param use_half_train (bool): return half the data or the whole train data\n",
    "    @param batch_size (int): batch size for training and testing\n",
    "    @param dataset_portion (double): portion of train data\n",
    "\n",
    "    @returns dict() with train and test data loaders with keys `train_loader`, `test_loader`\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalization using channel means\n",
    "    normalize_transform = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "    # Creating transform function\n",
    "    train_transform = transforms.Compose([transforms.ToTensor(), normalize_transform])\n",
    "        \n",
    "    # Test transformation function    \n",
    "    test_transform = transforms.Compose([transforms.ToTensor(), normalize_transform])\n",
    "    \n",
    "    # Check which dataset is required and load data from torchvision datasets\n",
    "    if dataset == 'cifar10':\n",
    "        original_train_dataset = datasets.CIFAR10(root=os.path.join('data', 'cifar10_data'),\n",
    "                                             train=True, transform=train_transform, download=True)\n",
    "        original_test_dataset = datasets.CIFAR10(root=os.path.join('data', 'cifar10_data'),\n",
    "                                             train=False, transform=test_transform, download=True)\n",
    "    elif dataset == 'cifar100':\n",
    "        original_train_dataset = datasets.CIFAR100(root=os.path.join('data', 'cifar100_data'),\n",
    "                                             train=True, transform=train_transform, download=True)\n",
    "        original_test_dataset = datasets.CIFAR100(root=os.path.join('data', 'cifar100_data'),\n",
    "                                             train=False, transform=test_transform, download=True)\n",
    "    else:\n",
    "        original_train_dataset = datasets.SVHN(root=os.path.join('data', 'svhn_data'),\n",
    "                                             split='train', transform=train_transform, download=True)\n",
    "        original_test_dataset = datasets.SVHN(root=os.path.join('data', 'svhn_data'),\n",
    "                                             split='test', transform=test_transform, download=True)\n",
    "    \n",
    "    # Check half data flag\n",
    "    if use_half_train:\n",
    "        print('Using Half Data')\n",
    "        dataset_portion = 0.5\n",
    "        \n",
    "    # Check if only a portion is required\n",
    "    if dataset_portion:\n",
    "        dataset_size = len(original_train_dataset)\n",
    "        split = int(np.floor((1 - dataset_portion) * dataset_size))\n",
    "        original_train_dataset, _ = random_split(original_train_dataset, [dataset_size - split, split])\n",
    "    \n",
    "    # Creating data loaders\n",
    "    loader_args = {\n",
    "        \"batch_size\": batch_size,\n",
    "    }\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=original_train_dataset,\n",
    "        shuffle=True,\n",
    "        **loader_args)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=original_test_dataset,\n",
    "        shuffle=False,\n",
    "        **loader_args)\n",
    "\n",
    "    return {\"train_loader\": train_loader,\n",
    "            \"test_loader\": test_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25be4c0d-5e04-43e4-b167-22c54f28bd0e",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "The following function is the `train_model_exp1` which trains a ResNet-18 model on the CIFAR-10 dataset and returns the train and test accuracies. The function takes six parameters:\n",
    "\n",
    "-   `title`: a string that specifies the name of the experiment. This is used to create a subdirectory under the `experiments/exp1` directory where the model checkpoints and final weights will be saved.\n",
    "-   `experiment_dir`: a string that specifies the path of the experiment directory. If this is `None`, then the function will use the title parameter to create a default directory name.\n",
    "-   `use_half_data`: a boolean flag that indicates whether to use half of the training data or the whole dataset. This is passed to the `get_loaders` function that loads the data loaders.\n",
    "-   `lr`: a float value that specifies the learning rate for the stochastic gradient descent optimizer.\n",
    "-   `checkpoint`: a string that specifies the path of a model checkpoint file. If this is not `None`, then the function will load the model weights from the checkpoint file and resume training from there.\n",
    "-   `epochs`: an integer that specifies the number of epochs to train the model for.\n",
    "\n",
    "The function returns a tuple of two lists: `train_acc` and `test_acc`. These are lists that contain the train and test accuracies for each epoch, respectively. The function uses the [ResNet18](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html) model from the torchvision models. The function also sets the random seeds for reproducibility. The function uses cross entropy loss as the loss function and SGD as an optimizer. The function also uses a helper function called get_accuracy to compute the accuracy of the model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0446fecd-342b-4843-8f3f-bf2354505e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function takes predictions and true values to return accuracies\n",
    "def get_accuracy(logit, true_y):\n",
    "    pred_y = torch.argmax(logit, dim=1)\n",
    "    return (pred_y == true_y).float().mean()\n",
    "\n",
    "# Function to train the model and return train and test accuracies\n",
    "def train_model_exp1(title='', experiment_dir=None, use_half_data=False, lr=0.001, checkpoint=None, epochs=10):\n",
    "    # Create experiment directory name if none\n",
    "    if experiment_dir is None:\n",
    "        experiment_dir = os.path.join('experiments/exp1', title)\n",
    "\n",
    "    # make experiment directory\n",
    "    os.makedirs(experiment_dir, exist_ok=True)\n",
    "\n",
    "    # Set the seed\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Check if GPU is available\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "        print(\"CUDA Recognized\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    # Get the dataset\n",
    "    loaders = get_loaders(dataset=\"cifar10\", use_half_train=use_half_data)\n",
    "    num_classes = 10\n",
    "\n",
    "    # Get the model\n",
    "    model = models.resnet18(num_classes=10).to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    # Create the loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Get model from checkpoint\n",
    "    if checkpoint is not None:\n",
    "        model.load_state_dict(torch.load(checkpoint, map_location=device)['model'])\n",
    "\n",
    "    # Arrays to hold accuracies\n",
    "    test_acc = [0]\n",
    "    train_acc = [0]\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "\n",
    "        for batch_idx, (data_x, data_y) in enumerate(loaders[\"train_loader\"]):\n",
    "            data_x = data_x.to(device)\n",
    "            data_y = data_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model_y = model(data_x)\n",
    "            loss = criterion(model_y, data_y)\n",
    "            batch_accuracy = get_accuracy(model_y, data_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            accuracies.append(batch_accuracy.item())\n",
    "            losses.append(loss.item())\n",
    "\n",
    "\n",
    "        train_loss = np.mean(losses)\n",
    "        train_accuracy = np.mean(accuracies)\n",
    "        train_acc.append(train_accuracy*100)\n",
    "\n",
    "        print(\"Train accuracy: {} Train loss: {}\".format(train_accuracy, train_loss))\n",
    "\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        model.eval()\n",
    "        for batch_idx, (data_x, data_y) in enumerate(loaders[\"test_loader\"]):\n",
    "            data_x = data_x.to(device)\n",
    "            data_y = data_y.to(device)\n",
    "\n",
    "            model_y = model(data_x)\n",
    "            loss = criterion(model_y, data_y)\n",
    "            batch_accuracy = get_accuracy(model_y, data_y)\n",
    "\n",
    "            accuracies.append(batch_accuracy.item())\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        test_loss = np.mean(losses)\n",
    "        test_accuracy = np.mean(accuracies)\n",
    "        test_acc.append(test_accuracy*100)\n",
    "        print(\"Test accuracy: {} Test loss: {}\".format(test_accuracy, test_loss))\n",
    "\n",
    "\n",
    "    torch.save({\n",
    "        'model': model.state_dict()\n",
    "    }, os.path.join(experiment_dir, 'final.pt'))\n",
    "    \n",
    "    # return the accuracies\n",
    "    return train_acc, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05b2112-5121-4c62-beb3-9aa41cd36191",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "To be used warm-starting a model later, we first train a model for 350 epochs on 50% of the CIFAR-10 dataset. We keep track of the train and test accuracies at each epoch, which will form the blue line on the left half of figure 1.\n",
    "\n",
    "We set `use_half_data` to `True` to train on only half of the CIFAR-10 dataset. We don’t need a `checkpoint` since we start from scratch. We use `lr = 0.001` for all models, following the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e38efd-fc25-4fcf-85a1-85dd9b3dc83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize runs dictionary to hold runs outputs\n",
    "runs = {}\n",
    "\n",
    "# Run the train_model_exp1 function to get train and test accuracies for the first model (trained on half the data)\n",
    "half_cifar_train_acc, half_cifar_test_acc = train_model_exp1( title=\"half_cifar\",\n",
    "                                                                use_half_data=True,\n",
    "                                                                lr=0.001,\n",
    "                                                                checkpoint=None,\n",
    "                                                                epochs=350 )\n",
    "\n",
    "# Put the results in the runs dictionary\n",
    "runs[\"half_cifar\"] = [half_cifar_train_acc, half_cifar_test_acc, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275ea762-b30b-4ad5-9b20-5fbfee3533b3",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "Now we use the previous model to train a warm-starting model for 350 epochs on 100% of the CIFAR-10 dataset. We keep track of the train and test accuracies at each epoch, which will form the blue line on the right half of figure 1.\n",
    "\n",
    "We set `use_half_data` to `False` to train on the full CIFAR-10 dataset. We add a `checkpoint` to the model trained on 50% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c92886-9d08-42a7-b510-726ae54c9622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the train_model_exp1 function to get train and test accuracies for the Second model ( warm starting )\n",
    "warm_start_train_acc, warm_start_test_acc = train_model_exp1( title=\"warm_start\",\n",
    "                                                                use_half_data=False,\n",
    "                                                                lr=0.001,\n",
    "                                                                checkpoint='experiments/exp1/half_cifar/final.pt',\n",
    "                                                                epochs=350 )\n",
    "\n",
    "# Put the results in the runs dictionary\n",
    "runs[\"warm_start\"] = [warm_start_train_acc, warm_start_test_acc, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197e8d0e-64e9-4c3a-9a7d-cfcbb1b06f21",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "Finaly, we train a model for 350 epochs on 100% of the CIFAR-10 dataset. We keep track of the train and test accuracies at each epoch, which will form the orange line on the right half of figure 1.\n",
    "\n",
    "We set `use_half_data` to `False` to train on the full CIFAR-10 dataset. We don’t need a `checkpoint` since we start from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd0592e-39fa-4b50-b6ff-42ed5ba17ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the train_model_exp1 function to get train and test accuracies for the Last model ( randomly initialized )\n",
    "full_cifar_train_acc, full_cifar_test_acc = train_model_exp1( title=\"full_cifar\",\n",
    "                                                                use_half_data=False,\n",
    "                                                                lr=0.001,\n",
    "                                                                checkpoint=None,\n",
    "                                                                epochs=350 )\n",
    "\n",
    "# Put the results in the runs dictionary\n",
    "runs[\"full_cifar\"] = [full_cifar_train_acc, full_cifar_test_acc, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00080dbc-a3ef-43c5-bba8-f52e544775f1",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "Now we save the training and test accuracies in the runs dictionary in `runs.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e66b043-3338-4cfd-85b0-df65174b7b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outputs in a json file\n",
    "with open(\"experiments/exp1/runs.json\", \"w\") as f:\n",
    "    json.dump(runs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5326a250-3078-4165-ad42-990b81e3d263",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "Let’s visualize the accuracies and analyze the outcomes! Run the next cell to plot the accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcf7362-3e8b-4152-b7a4-e48a51b08d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from json file\n",
    "with open(\"experiments/exp1/runs.json\", \"r\") as f:\n",
    "    runs = json.load(f)\n",
    "\n",
    "# Get number of epochs\n",
    "epochs = len(list(runs.items())[0][1][0])\n",
    "\n",
    "# Select colors\n",
    "colors = {'half_cifar' : 'C0',\n",
    "          'warm_start' : 'C0',\n",
    "          'full_cifar' : 'C1',\n",
    "         }\n",
    "\n",
    "# Plot train Figure\n",
    "plt.figure()\n",
    "for title, vals in runs.items():\n",
    "    offset = epochs * vals[2]\n",
    "    x = np.arange(offset, offset + len(vals[0]))\n",
    "    y = vals[0]\n",
    "    plt.plot(x, y, label=title, c=colors[title])\n",
    "plt.legend()\n",
    "\n",
    "plt.ylabel(\" Train accuracy \")\n",
    "plt.ylim(0, 100)\n",
    "plt.plot([epochs, epochs], plt.gca().get_ylim(), '--', c='black')\n",
    "plt.savefig(\"experiments/exp1/fig1_train.png\")\n",
    "\n",
    "# Plot test Figure\n",
    "plt.figure()\n",
    "for title, vals in runs.items():\n",
    "    offset = epochs * vals[2]\n",
    "    x = np.arange(offset, offset + len(vals[1]))\n",
    "    y = vals[1]\n",
    "    plt.plot(x, y, label=title, c=colors[title])\n",
    "plt.legend()\n",
    "\n",
    "plt.ylabel(\" Test accuracy \")\n",
    "plt.ylim(0, 100)\n",
    "plt.plot([epochs, epochs], plt.gca().get_ylim(), '--', c='black')\n",
    "plt.savefig(\"experiments/exp1/fig1_test.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0b442b-e063-474d-b52c-361877a6980c",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "<p style=\"color: crimson;font-size: 16px;\">\n",
    "Did the experiment description provide all the parameter values or did we make any assumptions? If so, what criteria do you think was used to make those assumptions?\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc11eeb-4a58-4a66-86fc-1e3489e48033",
   "metadata": {},
   "source": [
    "<p style=\"color: green; font-size: 16px;\">\n",
    "Answer:\n",
    "</p>\n",
    "\n",
    "------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0120d7-197b-4e5e-ba61-e0db59ea7baa",
   "metadata": {},
   "source": [
    "### Things to try:\n",
    "\n",
    "This experiment uses a specific model and optimizer. Exploring different combinations might be beneficial but costly in terms of computation. A simple way to further examine the first claim is:\n",
    "\n",
    "-   Use a lower learning rate since the model achieves 99% training accuracy quickly\n",
    "-   Use number of epochs at which validation accuracies of both models are maximized\n",
    "\n",
    "------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843404f6-963d-43b8-8e95-d6550f3d208b",
   "metadata": {},
   "source": [
    "If you are using colab click on this link to go to the next notebook: [Open in Colab](https://colab.research.google.com/github/mohammed183/ml-reproducibility-p1/blob/main/notebooks/04-Experiment2.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
