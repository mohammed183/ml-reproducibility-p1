{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On Warm-Starting Neural Network Training\n",
    "\n",
    "The paper is available on [arXiv](https://arxiv.org/abs/1910.08475). In creating the interactive material for this notebook, we utilized the code from this reproducibility challenge: [Re: Warm-Starting Neural Network Training](https://rescience.github.io/bibliography/Kireev_2021.html)."
   ],
   "id": "0b2c16b8-4391-4c27-ac68-7ea26a983c0a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Retraining neural networks with new data added to the training set is a time and energy-consuming task. To speed up this process, the technique of warm-starting can be used. Warm-starting involves using the weights of a pre-trained model, trained on a subset of the data, as the starting point for training the complete dataset.\n",
    "\n",
    "The paper examines the impact of warm-starting on the final model’s accuracy and highlights the presence of a generalization gap in warm-started models. The authors propose a method to address this gap by shrinking the pre-trained weights and introducing a random perturbation.\n",
    "\n",
    "The warm-starting technique is an effective way to accelerate the training process of large neural network models. However, the paper emphasizes the importance of mitigating the generalization gap in warm-started models, and proposes a method to achieve better accuracy.\n",
    "\n",
    "Updating datasets over time can be a costly endeavor, making it impractical to retrain models from scratch each time. Therefore, warm-starting becomes crucial as it allows leveraging pre-trained weights on a subset of the data, significantly reducing the time and resources required for training. By utilizing warm-starting, models can be efficiently adapted to incorporate new data without incurring the high computational expenses associated with starting from scratch."
   ],
   "id": "cd5d35fd-f058-43ac-99ae-c0180d47f2dc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### While experimenting we need to answer some questions to understand the level of reproducibility of this paper:\n",
    "\n",
    "-   Is there code available for training? for inference?\n",
    "-   Is it author code, or written by someone else? Are there multiple implementations available?\n",
    "-   What framework and version was used? Are all the functions are still available or should you make some changes?\n",
    "-   Did the author compare to other models that are not implemented in the code? Are these models available?\n",
    "-   Are all hyperparameters for all experiments available? If not, what is the sensitivity of each hyperparameter?\n",
    "-   Was the initial values set at random?\n",
    "-   Are the datasets used available? Are there any modifications done to the data?\n",
    "-   Did our results match that of the original paper?"
   ],
   "id": "1d1fffb7-b186-4f3c-b469-c16ccb655288"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claims by the authors:\n",
    "\n",
    "-   Claim 1: Warm-starting neural network training can reduce the resource usage associated with the construction of performant deep learning systems.\n",
    "-   Claim 2: Warm-starting neural network training can yield poorer generalization performance than models that have fresh random initializations, even though the final training losses are similar.\n",
    "-   Claim 3: Warm-starting neural network training can be improved by using a simple trick that involves resetting the batch normalization statistics after copying the weights from a previous model.\n",
    "-   Claim 4: The simple trick can close the generalization gap between warm-starting and random initialization in several important situations, such as when data arrive piecemeal, when data are actively selected, or when data are noisy or corrupted.\n",
    "-   Claim 5: The simple trick can also improve the convergence speed and stability of warm-starting, especially when the learning rate is large or the batch size is small.\n",
    "-   Claim 6: The reason why warm-starting can hurt generalization is that it can cause a mismatch between the batch normalization statistics and the data distribution, which can lead to suboptimal feature representations and gradient directions.\n",
    "-   Claim 7: The reason why resetting the batch normalization statistics can mitigate this effect is that it can restore the alignment between the batch normalization statistics and the data distribution, which can lead to better feature representations and gradient directions.\n",
    "\n",
    "Additional points:\n",
    "\n",
    "-   Training a model initialized with the weights trained on a part of the same dataset leads to loss of generality in the deep neural network, identified as the warm-starting gap. A model trained on 100% of the data at once takes more time to train but yields better results.\n",
    "-   The warm-starting gap is independent of batch size and learning rate.\n",
    "-   Only a little training with a warm-starting model can lead to a loss of generality.\n",
    "-   Regularization doesn’t resolve the generalization gap.\n",
    "-   Shrinking the weights doesn’t significantly affect models without bias or batch normalization, but extreme shrinking can impact the performance of more sophisticated architectures.\n",
    "-   Adding perturbation (noise) after shrinking improves both training time and generalization performance.\n",
    "-   Utilizing the shrink-perturb trick can close the generalization gap and provide similar results to a newly randomly initialized model in less training time."
   ],
   "id": "0871af85-f3af-4e5e-a09d-15ab7162215e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conducting Experiments to Test Previous Claims\n",
    "\n",
    "In this notebook, we will perform experiments to validate the claims mentioned earlier. Please note that some parts of the experiments will be incomplete, requiring you to fill in the missing functions with the correct parameter values as mentioned in the paper.\n",
    "\n",
    "For the values that are not explicitly provided by the authors, you can try different values to assess the sensitivity of the hyperparameters used.\n",
    "\n",
    "The missing code for the experiments can be found in the `solution.ipynb` notebook."
   ],
   "id": "179221cc-40d1-416f-9c18-1480b9d56d48"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "The following part contains data loaders, models, and training functions that will be used. Please note that you are not required to implement any of these functions, but rather read the notes on how to use them and understand their purpose."
   ],
   "id": "21d35df6-065a-4053-9dac-45b7463ab9c3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders\n",
    "\n",
    "The paper utilizes publicly available datasets, including CIFAR10, CIFAR100, and SVHN. We will create data loaders that match the specifications of the data used in the paper."
   ],
   "id": "c618bbdd-0561-4087-82e3-d04783711d69"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The next few cell should contain the code with explaination on how to use the dataloaders. \n",
    "## The only functions the students will need to run are the training functions at the end however I will be adding\n",
    "## explaination to everything."
   ],
   "id": "248c4416-34af-4455-956f-8a96352bdc55"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models:\n",
    "\n",
    "The model used in the paper are Logistic Regression, 3-Layer Resnet-18 and Multi-layer perceptron."
   ],
   "id": "30f81d1d-e0db-4916-9920-4951ff005f4e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The next few cells will contain the model implementations as function to be called in the training. \n",
    "## Explaination on the architecture and maybe some references will be added just in case anyone is curious to \n",
    "## understand the model architecture."
   ],
   "id": "3ba236ea-d274-4f58-80e6-a9fcd9bb145f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training:\n",
    "\n",
    "The training function will be added in this section, there are different training functions for different experiments.\n",
    "\n",
    "I will add explaination on to use these fucntions and give some examples as the students will have to use these functions."
   ],
   "id": "3371f953-4a8d-4d1a-b638-ff778f6dc8ee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The next few cells will contain the function with detailed explaination on how to use them"
   ],
   "id": "e808a6d9-7685-4e26-b076-12561593e952"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "In this section, we will utilize the previously defined functions to test the claims made by the authors. You will come across sections in the code marked with `#TODO`, where you need to add one or two functions.\n",
    "\n",
    "Please use the hyperparameter values provided in the paper. If certain hyperparameters are not provided, feel free to use your own values."
   ],
   "id": "30f67d41-ef52-4e16-9d3d-d1fde1de2ac2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment n: Testing Claim m\n",
    "\n",
    "In this experiment, we aim to validate the m’th claim mentioned in the paper. The authors have provided a specific section and a corresponding figure to support their claim.\n",
    "\n",
    "Our objective is to reproduce the figure and then compare it with the original one.\n",
    "\n",
    "To accomplish this, we need to identify the hyperparameter values required for this experiment and include them in the training function.\n",
    "\n",
    "Hint:\n",
    "\n",
    "-   Utilize the `train1` function for this particular experiment.\n",
    "-   Run the `plot_fig#` function in the subsequent cell.\n",
    "\n",
    "(I will add assertions to ensure the proper utilization of resources and to detect any potential issues in the process.)"
   ],
   "id": "0090b3ec-cdf4-4255-ae01-305993dc3713"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example on the student should write\n",
    "# TODO\n",
    "params = get_params(the hyperparameter values he chooses)\n",
    "returned_values_if_any = train(params)\n",
    "# End"
   ],
   "id": "4049ab9f-3dfa-47bb-9c2c-c972815af5e3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fig#()"
   ],
   "id": "eceb5fb0-82fa-487c-a92a-090b1c73de54"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the figure I will the figure that I got when doing this myself so they can make sure the one they got is right. I will also add some notes about the results, whether they match or not and so.\n",
    "\n",
    "#### The previous is repeated for all claims"
   ],
   "id": "be0bf09b-c89a-437f-8ea9-09782b76b604"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this section we will answer the questions in the begining of the notebook and maybe leave some room for the student to add his answers"
   ],
   "id": "0e22bad5-a239-4c0b-ae18-81a224a56fbc"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
